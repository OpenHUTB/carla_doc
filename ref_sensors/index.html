<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>传感器参考 - 驾驶仿真器文档</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../extra.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "\u4f20\u611f\u5668\u53c2\u8003";
        var mkdocs_page_input_path = "ref_sensors.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> 驾驶仿真器文档
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">主页</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">入门</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../start_introduction/">介绍</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../start_quickstart/">快速启动包安装</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tuto_first_steps/">第一步</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">构建 Carla</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../build_carla/">构建 Carla</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../build_linux/">Linux 构建</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../build_windows/">Windows 构建</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../build_update/">更新 Carla</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../build_system/">构建系统</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../build_docker/">Docker 中的 Carla</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../build_faq/">常问的问题</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">下一步</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../tuto_content_authoring_maps/">内容创作 - 地图</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tuto_content_authoring_vehicles/">内容创作 - 车辆</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Carla 主题</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../foundations/">基础</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../core_concepts/">核心概念</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../core_world/">世界和客户端</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../core_actors/">参与者和蓝图</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../core_map/">地图和导航 </a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../core_sensors/">传感器和数据</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../ts_traffic_simulation_overview/">交通</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../development_tutorials/">开发</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../custom_assets_tutorials/">自定义资产</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">高级概念</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../adv_recorder/">记录器</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../adv_rendering_options/">渲染选项</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../adv_synchrony_timestep/">同步和时间步长</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../adv_benchmarking/">基准性能</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../adv_agents/">Carla 智能体</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">交通仿真</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../adv_traffic_manager/">交通管理器</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../adv_sumo/">SUMO 联合仿真</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tuto_G_scenic/">Scenic</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">资源</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../python_api/">Python API 参考</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../ref_cpp/">C++ 参考</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../bp_library/">蓝图库</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../catalogue/">Carla 目录</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../ref_recorder_binary_file_format/">记录器二进制文件格式</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="./">传感器参考</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#collision-detector">Collision detector</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#output-attributes">Output attributes</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#depth-camera">Depth camera</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#basic-camera-attributes">Basic camera attributes</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#camera-lens-distortion-attributes">Camera lens distortion attributes</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#output-attributes_1">Output attributes</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#gnss-sensor">GNSS sensor</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#gnss-attributes">GNSS attributes</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#output-attributes_2">Output attributes</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#imu-sensor">IMU sensor</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#imu-attributes">IMU attributes</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#output-attributes_3">Output attributes</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#lane-invasion-detector">Lane invasion detector</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#output-attributes_4">Output attributes</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#lidar-sensor">LIDAR sensor</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#lidar-attributes">Lidar attributes</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#output-attributes_5">Output attributes</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#obstacle-detector">Obstacle detector</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#output-attributes_6">Output attributes</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#radar-sensor">Radar sensor</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#output-attributes_7">Output attributes</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#rgb-camera">RGB camera</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#basic-camera-attributes_1">Basic camera attributes</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#camera-lens-distortion-attributes_1">Camera lens distortion attributes</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#advanced-camera-attributes">Advanced camera attributes</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#output-attributes_8">Output attributes</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#rss-sensor">RSS sensor</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#the-carlarsssensor-class">The carla.RssSensor class</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#output-attributes_9">Output attributes</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semantic-lidar-sensor">Semantic LIDAR sensor</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#semanticlidar-attributes">SemanticLidar attributes</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#output-attributes_10">Output attributes</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#semantic-segmentation-camera">Semantic segmentation camera</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#basic-camera-attributes_2">Basic camera attributes</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#camera-lens-distortion-attributes_2">Camera lens distortion attributes</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#output-attributes_11">Output attributes</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#dvs-camera">DVS camera</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#dvs-camera-attributes">DVS camera attributes</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#optical-flow-camera">Optical Flow Camera</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#optical-flow-camera-attributes">Optical Flow camera attributes</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#optical-flow-camera-lens-distortion-attributes">Optical Flow camera lens distortion attributes</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#output-attributes_12">Output attributes</a>
    </li>
        </ul>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tutorials/">教程</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../ext_docs/">扩展文档</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">插件</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../plugins_carlaviz/">carlaviz — web 可视化器</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">ROS 桥接器</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../ros_documentation/">ROS 桥文档</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">自定义地图</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../tuto_M_custom_map_overview/">Carla 中自定义地图的概述</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tuto_M_generate_map/">在 RoadRunner 中创建地图</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tuto_M_add_map_package/">在CARLA包导入地图</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tuto_M_add_map_source/">在 Carla 源构建中导入地图</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tuto_M_add_map_alternative/">导入地图的替代方法</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tuto_M_manual_map_package/">手动准备地图包</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tuto_M_custom_layers/">自定义地图：分层地图: </a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tuto_M_custom_add_tl/">自定义地图：红绿灯和标志</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tuto_M_custom_road_painter/">自定义地图：Road painter</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tuto_M_custom_buildings/">自定义地图：程序建筑</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tuto_M_custom_weather_landscape/">自定义地图：天气和景观</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tuto_M_generate_pedestrian_navigation/">生成行人导航</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">大型地图</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../large_map_overview/">大型地图概述</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../large_map_roadrunner/">在RoadRunner中创建大地图</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../large_map_import/">导入/打包大地图</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">教程（通用）</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../tuto_G_add_friction_triggers/">添加摩擦触发器</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tuto_G_control_vehicle_physics/">控制车辆物理模型</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tuto_G_control_walker_skeletons/">控制行人骨骼</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tuto_G_openstreetmap/">使用 OpenStreetMap 生成地图</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tuto_G_retrieve_data/">检索模拟数据</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../build_docker_unreal/">在 Docker 中构建虚幻引擎和 Carla</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">教程（资产）</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../tuto_A_add_vehicle/">添加新车辆</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tuto_A_add_props/">添加新道具</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tuto_A_create_standalone/">创建独立包</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tuto_A_material_customization/">材料定制</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">教程（开发人员）</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../tuto_D_contribute_assets/">如何升级内容</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tuto_D_create_sensor/">创建一个传感器</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tuto_D_create_semantic_tags/">创建语义标签</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tuto_D_customize_vehicle_suspension/">自定义车辆悬架</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tuto_D_generate_colliders/">生成详细碰撞</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tuto_D_make_release/">发布版本</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">Carla 生态系统</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../ecosys_ansys/">Ansys 实时雷达模型</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tuto_G_rllib_integration/">RLlib 集成</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tuto_G_carsim_integration/">CarSim 集成</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tuto_G_chrono/">Chrono 集成</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../adv_opendrive/">OpenDRIVE 独立模式</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../adv_ptv/">PTV-Vissim 联合仿真</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../adv_rss/">RSS</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="https://carla.readthedocs.io/projects/ros-bridge/en/latest/">ROS</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">贡献</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../cont_contribution_guidelines/">贡献指南</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../cont_code_of_conduct/">行为准则</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../cont_coding_standard/">编码标准</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../cont_doc_standard/">文档标准</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">驾驶仿真器文档</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">资源</li>
      <li class="breadcrumb-item active">传感器参考</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="sensors-reference">Sensors reference</h1>
<ul>
<li><a href="#collision-detector"><strong>Collision detector</strong></a></li>
<li><a href="#depth-camera"><strong>Depth camera</strong></a></li>
<li><a href="#gnss-sensor"><strong>GNSS sensor</strong></a></li>
<li><a href="#imu-sensor"><strong>IMU sensor</strong></a></li>
<li><a href="#lane-invasion-detector"><strong>Lane invasion detector</strong></a></li>
<li><a href="#lidar-sensor"><strong>LIDAR sensor</strong></a></li>
<li><a href="#obstacle-detector"><strong>Obstacle detector</strong></a></li>
<li><a href="#radar-sensor"><strong>Radar sensor</strong></a></li>
<li><a href="#rgb-camera"><strong>RGB camera</strong></a></li>
<li><a href="#rss-sensor"><strong>RSS sensor</strong></a></li>
<li><a href="#semantic-lidar-sensor"><strong>Semantic LIDAR sensor</strong></a></li>
<li><a href="#semantic-segmentation-camera"><strong>Semantic segmentation camera</strong></a></li>
<li><a href="#dvs-camera"><strong>DVS camera</strong></a></li>
<li><a href="#optical-flow-camera"><strong>Optical Flow camera</strong></a></li>
</ul>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>All the sensors use the UE coordinate system (<strong>x</strong>-<em>forward</em>, <strong>y</strong>-<em>right</em>, <strong>z</strong>-<em>up</em>), and return coordinates in local space. When using any visualization software, pay attention to its coordinate system. Many invert the Y-axis, so visualizing the sensor data directly may result in mirrored outputs.  </p>
</div>
<hr />
<h2 id="collision-detector">Collision detector</h2>
<ul>
<li><strong>Blueprint:</strong> sensor.other.collision</li>
<li><strong>Output:</strong> <a href="../python_api/#carla.CollisionEvent">carla.CollisionEvent</a> per collision.</li>
</ul>
<p>This sensor registers an event each time its parent actor collisions against something in the world. Several collisions may be detected during a single simulation step.
To ensure that collisions with any kind of object are detected, the server creates "fake" actors for elements such as buildings or bushes so the semantic tag can be retrieved to identify it.</p>
<p>Collision detectors do not have any configurable attribute.</p>
<h4 id="output-attributes">Output attributes</h4>
<table>
<thead>
<tr>
<th>Sensor data attribute</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>frame</code></td>
<td>int</td>
<td>Frame number when the measurement took place.</td>
</tr>
<tr>
<td><code>timestamp</code></td>
<td>double</td>
<td>Simulation time of the measurement in seconds since the beginning of the episode.</td>
</tr>
<tr>
<td><code>transform</code></td>
<td><a href="../python_api#carlatransform">carla.Transform</a></td>
<td>Location and rotation in world coordinates of the sensor at the time of the measurement.</td>
</tr>
<tr>
<td><code>actor</code></td>
<td><a href="../python_api#carlaactor">carla.Actor</a></td>
<td>Actor that measured the collision (sensor's parent).</td>
</tr>
<tr>
<td><code>other_actor</code></td>
<td><a href="../python_api#carlaactor">carla.Actor</a></td>
<td>Actor against whom the parent collided.</td>
</tr>
<tr>
<td><code>normal_impulse</code></td>
<td><a href="../python_api#carlavector3d">carla.Vector3D</a></td>
<td>Normal impulse result of the collision.</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="depth-camera">Depth camera</h2>
<ul>
<li><strong>Blueprint:</strong> sensor.camera.depth</li>
<li><strong>Output:</strong> <a href="../python_api/#carla.Image">carla.Image</a> per step (unless <code>sensor_tick</code> says otherwise).</li>
</ul>
<p>The camera provides a raw data of the scene codifying the distance of each pixel to the camera (also known as <strong>depth buffer</strong> or <strong>z-buffer</strong>) to create a depth map of the elements.</p>
<p>The image codifies depth value per pixel using 3 channels of the RGB color space, from less to more significant bytes: <em>R -&gt; G -&gt; B</em>. The actual distance in meters can be
decoded with:</p>
<pre><code>normalized = (R + G * 256 + B * 256 * 256) / (256 * 256 * 256 - 1)
in_meters = 1000 * normalized
</code></pre>
<p>The output <a href="../python_api/#carla.Image">carla.Image</a> should then be saved to disk using a <a href="../python_api/#carla.ColorConverter">carla.colorConverter</a> that will turn the distance stored in RGB channels into a <strong>[0,1]</strong> float containing the distance and then translate this to grayscale.
There are two options in <a href="../python_api/#carla.ColorConverter">carla.colorConverter</a> to get a depth view: <strong>Depth</strong> and <strong>Logaritmic depth</strong>. The precision is milimetric in both, but the logarithmic approach provides better results for closer objects.</p>
<pre><code class="language-py">...
raw_image.save_to_disk(&quot;path/to/save/converted/image&quot;,carla.Depth)
</code></pre>
<p><img alt="ImageDepth" src="../img/ref_sensors_depth.jpg" /></p>
<h4 id="basic-camera-attributes">Basic camera attributes</h4>
<table>
<thead>
<tr>
<th>Blueprint attribute</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>image_size_x</code></td>
<td>int</td>
<td>800</td>
<td>Image width in pixels.</td>
</tr>
<tr>
<td><code>image_size_y</code></td>
<td>int</td>
<td>600</td>
<td>Image height in pixels.</td>
</tr>
<tr>
<td><code>fov</code></td>
<td>float</td>
<td>90.0</td>
<td>Horizontal field of view in degrees.</td>
</tr>
<tr>
<td><code>sensor_tick</code></td>
<td>float</td>
<td>0.0</td>
<td>Simulation seconds between sensor captures (ticks).</td>
</tr>
</tbody>
</table>
<h4 id="camera-lens-distortion-attributes">Camera lens distortion attributes</h4>
<table>
<thead>
<tr>
<th>Blueprint attribute</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>lens_circle_falloff</code></td>
<td>float</td>
<td>5.0</td>
<td>Range: [0.0, 10.0]</td>
</tr>
<tr>
<td><code>lens_circle_multiplier</code></td>
<td>float</td>
<td>0.0</td>
<td>Range: [0.0, 10.0]</td>
</tr>
<tr>
<td><code>lens_k</code></td>
<td>float</td>
<td>-1.0</td>
<td>Range: [-inf, inf]</td>
</tr>
<tr>
<td><code>lens_kcube</code></td>
<td>float</td>
<td>0.0</td>
<td>Range: [-inf, inf]</td>
</tr>
<tr>
<td><code>lens_x_size</code></td>
<td>float</td>
<td>0.08</td>
<td>Range: [0.0, 1.0]</td>
</tr>
<tr>
<td><code>lens_y_size</code></td>
<td>float</td>
<td>0.08</td>
<td>Range: [0.0, 1.0]</td>
</tr>
</tbody>
</table>
<h4 id="output-attributes_1">Output attributes</h4>
<table>
<thead>
<tr>
<th>Sensor data attribute</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>frame</code></td>
<td>int</td>
<td>Frame number when the measurement took place.</td>
</tr>
<tr>
<td><code>timestamp</code></td>
<td>double</td>
<td>Simulation time of the measurement in seconds since the beginning of the episode.</td>
</tr>
<tr>
<td><code>transform</code></td>
<td><a href="../python_api#carlatransform">carla.Transform</a></td>
<td>Location and rotation in world coordinates of the sensor at the time of the measurement.</td>
</tr>
<tr>
<td><code>width</code></td>
<td>int</td>
<td>Image width in pixels.</td>
</tr>
<tr>
<td><code>height</code></td>
<td>int</td>
<td>Image height in pixels.</td>
</tr>
<tr>
<td><code>fov</code></td>
<td>float</td>
<td>Horizontal field of view in degrees.</td>
</tr>
<tr>
<td><code>raw_data</code></td>
<td>bytes</td>
<td>Array of BGRA 32-bit pixels.</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="gnss-sensor">GNSS sensor</h2>
<ul>
<li><strong>Blueprint:</strong> sensor.other.gnss</li>
<li><strong>Output:</strong> <a href="../python_api/#carla.GnssMeasurement">carla.GNSSMeasurement</a> per step (unless <code>sensor_tick</code> says otherwise).</li>
</ul>
<p>Reports current <a href="https://www.gsa.europa.eu/european-gnss/what-gnss">gnss position</a> of its parent object. This is calculated by adding the metric position to an initial geo reference location defined within the OpenDRIVE map definition.</p>
<h4 id="gnss-attributes">GNSS attributes</h4>
<table>
<thead>
<tr>
<th>Blueprint attribute</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>noise_alt_bias</code></td>
<td>float</td>
<td>0.0</td>
<td>Mean parameter in the noise model for altitude.</td>
</tr>
<tr>
<td><code>noise_alt_stddev</code></td>
<td>float</td>
<td>0.0</td>
<td>Standard deviation parameter in the noise model for altitude.</td>
</tr>
<tr>
<td><code>noise_lat_bias</code></td>
<td>float</td>
<td>0.0</td>
<td>Mean parameter in the noise model for latitude.</td>
</tr>
<tr>
<td><code>noise_lat_stddev</code></td>
<td>float</td>
<td>0.0</td>
<td>Standard deviation parameter in the noise model for latitude.</td>
</tr>
<tr>
<td><code>noise_lon_bias</code></td>
<td>float</td>
<td>0.0</td>
<td>Mean parameter in the noise model for longitude.</td>
</tr>
<tr>
<td><code>noise_lon_stddev</code></td>
<td>float</td>
<td>0.0</td>
<td>Standard deviation parameter in the noise model for longitude.</td>
</tr>
<tr>
<td><code>noise_seed</code></td>
<td>int</td>
<td>0</td>
<td>Initializer for a pseudorandom number generator.</td>
</tr>
<tr>
<td><code>sensor_tick</code></td>
<td>float</td>
<td>0.0</td>
<td>Simulation seconds between sensor captures (ticks).</td>
</tr>
</tbody>
</table>
<p><br></p>
<h4 id="output-attributes_2">Output attributes</h4>
<table>
<thead>
<tr>
<th>Sensor data attribute</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>frame</code></td>
<td>int</td>
<td>Frame number when the measurement took place.</td>
</tr>
<tr>
<td><code>timestamp</code></td>
<td>double</td>
<td>Simulation time of the measurement in seconds since the beginning of the episode.</td>
</tr>
<tr>
<td><code>transform</code></td>
<td><a href="../python_api#carlatransform">carla.Transform</a></td>
<td>Location and rotation in world coordinates of the sensor at the time of the measurement.</td>
</tr>
<tr>
<td><code>latitude</code></td>
<td>double</td>
<td>Latitude of the actor.</td>
</tr>
<tr>
<td><code>longitude</code></td>
<td>double</td>
<td>Longitude of the actor.</td>
</tr>
<tr>
<td><code>altitude</code></td>
<td>double</td>
<td>Altitude of the actor.</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="imu-sensor">IMU sensor</h2>
<ul>
<li><strong>Blueprint:</strong> sensor.other.imu</li>
<li><strong>Output:</strong> <a href="../python_api/#carla.IMUMeasurement">carla.IMUMeasurement</a> per step (unless <code>sensor_tick</code> says otherwise).</li>
</ul>
<p>Provides measures that accelerometer, gyroscope and compass would retrieve for the parent object. The data is collected from the object's current state.</p>
<h4 id="imu-attributes">IMU attributes</h4>
<table>
<thead>
<tr>
<th>Blueprint attribute</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>noise_accel_stddev_x</code></td>
<td>float</td>
<td>0.0</td>
<td>Standard deviation parameter in the noise model for acceleration (X axis).</td>
</tr>
<tr>
<td><code>noise_accel_stddev_y</code></td>
<td>float</td>
<td>0.0</td>
<td>Standard deviation parameter in the noise model for acceleration (Y axis).</td>
</tr>
<tr>
<td><code>noise_accel_stddev_z</code></td>
<td>float</td>
<td>0.0</td>
<td>Standard deviation parameter in the noise model for acceleration (Z axis).</td>
</tr>
<tr>
<td><code>noise_gyro_bias_x</code></td>
<td>float</td>
<td>0.0</td>
<td>Mean parameter in the noise model for the gyroscope (X axis).</td>
</tr>
<tr>
<td><code>noise_gyro_bias_y</code></td>
<td>float</td>
<td>0.0</td>
<td>Mean parameter in the noise model for the gyroscope (Y axis).</td>
</tr>
<tr>
<td><code>noise_gyro_bias_z</code></td>
<td>float</td>
<td>0.0</td>
<td>Mean parameter in the noise model for the gyroscope (Z axis).</td>
</tr>
<tr>
<td><code>noise_gyro_stddev_x</code></td>
<td>float</td>
<td>0.0</td>
<td>Standard deviation parameter in the noise model for the gyroscope (X axis).</td>
</tr>
<tr>
<td><code>noise_gyro_stddev_y</code></td>
<td>float</td>
<td>0.0</td>
<td>Standard deviation parameter in the noise model for the gyroscope (Y axis).</td>
</tr>
<tr>
<td><code>noise_gyro_stddev_z</code></td>
<td>float</td>
<td>0.0</td>
<td>Standard deviation parameter in the noise model for the gyroscope (Z axis).</td>
</tr>
<tr>
<td><code>noise_seed</code></td>
<td>int</td>
<td>0</td>
<td>Initializer for a pseudorandom number generator.</td>
</tr>
<tr>
<td><code>sensor_tick</code></td>
<td>float</td>
<td>0.0</td>
<td>Simulation seconds between sensor captures (ticks).</td>
</tr>
</tbody>
</table>
<p><br></p>
<h4 id="output-attributes_3">Output attributes</h4>
<table>
<thead>
<tr>
<th>Sensor data attribute</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>frame</code></td>
<td>int</td>
<td>Frame number when the measurement took place.</td>
</tr>
<tr>
<td><code>timestamp</code></td>
<td>double</td>
<td>Simulation time of the measurement in seconds since the beginning of the episode.</td>
</tr>
<tr>
<td><code>transform</code></td>
<td><a href="../python_api#carlatransform">carla.Transform</a></td>
<td>Location and rotation in world coordinates of the sensor at the time of the measurement.</td>
</tr>
<tr>
<td><code>accelerometer</code></td>
<td><a href="../python_api#carlavector3d">carla.Vector3D</a></td>
<td>Measures linear acceleration in <code>m/s^2</code>.</td>
</tr>
<tr>
<td><code>gyroscope</code></td>
<td><a href="../python_api#carlavector3d">carla.Vector3D</a></td>
<td>Measures angular velocity in <code>rad/sec</code>.</td>
</tr>
<tr>
<td><code>compass</code></td>
<td>float</td>
<td>Orientation in radians. North is <code>(0.0, -1.0, 0.0)</code> in UE.</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="lane-invasion-detector">Lane invasion detector</h2>
<ul>
<li><strong>Blueprint:</strong> sensor.other.lane_invasion</li>
<li><strong>Output:</strong> <a href="../python_api/#carla.LaneInvasionEvent">carla.LaneInvasionEvent</a> per crossing.</li>
</ul>
<p>Registers an event each time its parent crosses a lane marking.
The sensor uses road data provided by the OpenDRIVE description of the map to determine whether the parent vehicle is invading another lane by considering the space between wheels.
However there are some things to be taken into consideration:</p>
<ul>
<li>Discrepancies between the OpenDRIVE file and the map will create irregularities such as crossing lanes that are not visible in the map.</li>
<li>The output retrieves a list of crossed lane markings: the computation is done in OpenDRIVE and considering the whole space between the four wheels as a whole. Thus, there may be more than one lane being crossed at the same time.</li>
</ul>
<p>This sensor does not have any configurable attribute.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>This sensor works fully on the client-side.</p>
</div>
<h4 id="output-attributes_4">Output attributes</h4>
<table>
<thead>
<tr>
<th>Sensor data attribute</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>frame</code></td>
<td>int</td>
<td>Frame number when the measurement took place.</td>
</tr>
<tr>
<td><code>timestamp</code></td>
<td>double</td>
<td>Simulation time of the measurement in seconds since the beginning of the episode.</td>
</tr>
<tr>
<td><code>transform</code></td>
<td><a href="../python_api#carlatransform">carla.Transform</a></td>
<td>Location and rotation in world coordinates of the sensor at the time of the measurement.</td>
</tr>
<tr>
<td><code>actor</code></td>
<td><a href="../python_api#carlaactor">carla.Actor</a></td>
<td>Vehicle that invaded another lane (parent actor).</td>
</tr>
<tr>
<td><code>crossed_lane_markings</code></td>
<td>list(<a href="../python_api#carlalanemarking">carla.LaneMarking</a>)</td>
<td>List of lane markings that have been crossed.</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="lidar-sensor">LIDAR sensor</h2>
<ul>
<li><strong>Blueprint:</strong> sensor.lidar.ray_cast</li>
<li><strong>Output:</strong> <a href="../python_api/#carla.LidarMeasurement">carla.LidarMeasurement</a> per step (unless <code>sensor_tick</code> says otherwise).</li>
</ul>
<p>This sensor simulates a rotating LIDAR implemented using ray-casting.
The points are computed by adding a laser for each channel distributed in the vertical FOV. The rotation is simulated computing the horizontal angle that the Lidar rotated in a frame. The point cloud is calculated by doing a ray-cast for each laser in every step.
<code>points_per_channel_each_step = points_per_second / (FPS * channels)</code></p>
<p>A LIDAR measurement contains a package with all the points generated during a <code>1/FPS</code> interval. During this interval the physics are not updated so all the points in a measurement reflect the same "static picture" of the scene.</p>
<p>This output contains a cloud of simulation points and thus, it can be iterated to retrieve a list of their <a href="../python_api/#carla.Location"><code>carla.Location</code></a>:</p>
<pre><code class="language-py">for location in lidar_measurement:
    print(location)
</code></pre>
<p>The information of the LIDAR measurement is enconded 4D points. Being the first three, the space points in xyz coordinates and the last one intensity loss during the travel. This intensity is computed by the following formula.
<br>
<img alt="LidarIntensityComputation" src="../img/lidar_intensity.jpg" /></p>
<p><code>a</code> — Attenuation coefficient. This may depend on the sensor's wavelenght, and the conditions of the atmosphere. It can be modified with the LIDAR attribute <code>atmosphere_attenuation_rate</code>.
<code>d</code> — Distance from the hit point to the sensor.</p>
<p>For a better realism, points in the cloud can be dropped off. This is an easy way to simulate loss due to external perturbations. This can done combining two different.</p>
<ul>
<li><strong>General drop-off</strong> — Proportion of points that are dropped off randomly. This is done before the tracing, meaning the points being dropped are not calculated, and therefore improves the performance. If <code>dropoff_general_rate = 0.5</code>, half of the points will be dropped.</li>
<li><strong>Instensity-based drop-off</strong> — For each point detected, and extra drop-off is performed with a probability based in the computed intensity. This probability is determined by two parameters. <code>dropoff_zero_intensity</code> is the probability of points with zero intensity to be dropped. <code>dropoff_intensity_limit</code> is a threshold intensity above which no points will be dropped. The probability of a point within the range to be dropped is a linear proportion based on these two parameters.</li>
</ul>
<p>Additionally, the <code>noise_stddev</code> attribute makes for a noise model to simulate unexpected deviations that appear in real-life sensors. For positive values, each point is randomly perturbed along the vector of the laser ray. The result is a LIDAR sensor with perfect angular positioning, but noisy distance measurement.</p>
<p>The rotation of the LIDAR can be tuned to cover a specific angle on every simulation step (using a <a href="../adv_synchrony_timestep/">fixed time-step</a>). For example, to rotate once per step (full circle output, as in the picture below), the rotation frequency and the simulated FPS should be equal. <br> <strong>1.</strong> Set the sensor's frequency <code>sensors_bp['lidar'][0].set_attribute('rotation_frequency','10')</code>. <br> <strong>2.</strong> Run the simulation using <code>python3 config.py --fps=10</code>.</p>
<p><img alt="LidarPointCloud" src="../img/lidar_point_cloud.jpg" /></p>
<h4 id="lidar-attributes">Lidar attributes</h4>
<table>
<thead>
<tr>
<th>Blueprint attribute</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>channels</code></td>
<td>int</td>
<td>32</td>
<td>Number of lasers.</td>
</tr>
<tr>
<td><code>range</code></td>
<td>float</td>
<td>10.0</td>
<td>Maximum distance to measure/raycast in meters (centimeters for CARLA 0.9.6 or previous).</td>
</tr>
<tr>
<td><code>points_per_second</code></td>
<td>int</td>
<td>56000</td>
<td>Points generated by all lasers per second.</td>
</tr>
<tr>
<td><code>rotation_frequency</code></td>
<td>float</td>
<td>10.0</td>
<td>LIDAR rotation frequency.</td>
</tr>
<tr>
<td><code>upper_fov</code></td>
<td>float</td>
<td>10.0</td>
<td>Angle in degrees of the highest laser.</td>
</tr>
<tr>
<td><code>lower_fov</code></td>
<td>float</td>
<td>-30.0</td>
<td>Angle in degrees of the lowest laser.</td>
</tr>
<tr>
<td><code>horizontal_fov</code></td>
<td>float</td>
<td>360.0</td>
<td>Horizontal field of view in degrees, 0 - 360.</td>
</tr>
<tr>
<td><code>atmosphere_attenuation_rate</code></td>
<td>float</td>
<td>0.004</td>
<td>Coefficient that measures the LIDAR instensity loss per meter. Check the intensity computation above.</td>
</tr>
<tr>
<td><code>dropoff_general_rate</code></td>
<td>float</td>
<td>0.45</td>
<td>General proportion of points that are randomy dropped.</td>
</tr>
<tr>
<td><code>dropoff_intensity_limit</code></td>
<td>float</td>
<td>0.8</td>
<td>For the intensity based drop-off, the threshold intensity value above which no points are dropped.</td>
</tr>
<tr>
<td><code>dropoff_zero_intensity</code></td>
<td>float</td>
<td>0.4</td>
<td>For the intensity based drop-off, the probability of each point with zero intensity being dropped.</td>
</tr>
<tr>
<td><code>sensor_tick</code></td>
<td>float</td>
<td>0.0</td>
<td>Simulation seconds between sensor captures (ticks).</td>
</tr>
<tr>
<td><code>noise_stddev</code></td>
<td>float</td>
<td>0.0</td>
<td>Standard deviation of the noise model to disturb each point along the vector of its raycast.</td>
</tr>
</tbody>
</table>
<h4 id="output-attributes_5">Output attributes</h4>
<table>
<thead>
<tr>
<th>Sensor data attribute</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>frame</code></td>
<td>int</td>
<td>Frame number when the measurement took place.</td>
</tr>
<tr>
<td><code>timestamp</code></td>
<td>double</td>
<td>Simulation time of the measurement in seconds since the beginning of the episode.</td>
</tr>
<tr>
<td><code>transform</code></td>
<td><a href="../python_api#carlatransform">carla.Transform</a></td>
<td>Location and rotation in world coordinates of the sensor at the time of the measurement.</td>
</tr>
<tr>
<td><code>horizontal_angle</code></td>
<td>float</td>
<td>Angle (radians) in the XY plane of the LIDAR in the current frame.</td>
</tr>
<tr>
<td><code>channels</code></td>
<td>int</td>
<td>Number of channels (lasers) of the LIDAR.</td>
</tr>
<tr>
<td><code>get_point_count(channel)</code></td>
<td>int</td>
<td>Number of points per channel captured this frame.</td>
</tr>
<tr>
<td><code>raw_data</code></td>
<td>bytes</td>
<td>Array of 32-bits floats (XYZI of each point).</td>
</tr>
</tbody>
</table>
<p><br></p>
<h2 id="obstacle-detector">Obstacle detector</h2>
<ul>
<li><strong>Blueprint:</strong> sensor.other.obstacle</li>
<li><strong>Output:</strong> <a href="../python_api/#carla.ObstacleDetectionEvent">carla.ObstacleDetectionEvent</a> per obstacle (unless <code>sensor_tick</code> says otherwise).</li>
</ul>
<p>Registers an event every time the parent actor has an obstacle ahead.
In order to anticipate obstacles, the sensor creates a capsular shape ahead of the parent vehicle and uses it to check for collisions.
To ensure that collisions with any kind of object are detected, the server creates "fake" actors for elements such as buildings or bushes so the semantic tag can be retrieved to identify it.</p>
<table>
<thead>
<tr>
<th>Blueprint attribute</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>distance</code></td>
<td>float</td>
<td>5</td>
<td>Distance to trace.</td>
</tr>
<tr>
<td><code>hit_radius</code></td>
<td>float</td>
<td>0.5</td>
<td>Radius of the trace.</td>
</tr>
<tr>
<td><code>only_dynamics</code></td>
<td>bool</td>
<td>False</td>
<td>If true, the trace will only consider dynamic objects.</td>
</tr>
<tr>
<td><code>debug_linetrace</code></td>
<td>bool</td>
<td>False</td>
<td>If true, the trace will be visible.</td>
</tr>
<tr>
<td><code>sensor_tick</code></td>
<td>float</td>
<td>0.0</td>
<td>Simulation seconds between sensor captures (ticks).</td>
</tr>
</tbody>
</table>
<p><br></p>
<h4 id="output-attributes_6">Output attributes</h4>
<table>
<thead>
<tr>
<th>Sensor data attribute</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>frame</code></td>
<td>int</td>
<td>Frame number when the measurement took place.</td>
</tr>
<tr>
<td><code>timestamp</code></td>
<td>double</td>
<td>Simulation time of the measurement in seconds since the beginning of the episode.</td>
</tr>
<tr>
<td><code>transform</code></td>
<td><a href="../python_api#carlatransform">carla.Transform</a></td>
<td>Location and rotation in world coordinates of the sensor at the time of the measurement.</td>
</tr>
<tr>
<td><code>actor</code></td>
<td><a href="../python_api#carlaactor">carla.Actor</a></td>
<td>Actor that detected the obstacle (parent actor).</td>
</tr>
<tr>
<td><code>other_actor</code></td>
<td><a href="../python_api#carlaactor">carla.Actor</a></td>
<td>Actor detected as an obstacle.</td>
</tr>
<tr>
<td><code>distance</code></td>
<td>float</td>
<td>Distance from <code>actor</code> to <code>other_actor</code>.</td>
</tr>
</tbody>
</table>
<p><br></p>
<h2 id="radar-sensor">Radar sensor</h2>
<ul>
<li><strong>Blueprint:</strong> sensor.other.radar</li>
<li><strong>Output:</strong> <a href="../python_api/#carla.RadarMeasurement">carla.RadarMeasurement</a> per step (unless <code>sensor_tick</code> says otherwise).</li>
</ul>
<p>The sensor creates a conic view that is translated to a 2D point map of the elements in sight and their speed regarding the sensor. This can be used to shape elements and evaluate their movement and direction. Due to the use of polar coordinates, the points will concentrate around the center of the view.</p>
<p>Points measured are contained in <a href="../python_api/#carla.RadarMeasurement">carla.RadarMeasurement</a> as an array of <a href="../python_api/#carla.RadarDetection">carla.RadarDetection</a>, which specifies their polar coordinates, distance and velocity.
This raw data provided by the radar sensor can be easily converted to a format manageable by <strong>numpy</strong>:</p>
<pre><code class="language-py"># To get a numpy [[vel, azimuth, altitude, depth],...[,,,]]:
points = np.frombuffer(radar_data.raw_data, dtype=np.dtype('f4'))
points = np.reshape(points, (len(radar_data), 4))
</code></pre>
<p>The provided script <code>manual_control.py</code> uses this sensor to show the points being detected and paint them white when static, red when moving towards the object and blue when moving away:</p>
<p><img alt="ImageRadar" src="../img/ref_sensors_radar.jpg" /></p>
<table>
<thead>
<tr>
<th>Blueprint attribute</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>horizontal_fov</code></td>
<td>float</td>
<td>30.0</td>
<td>Horizontal field of view in degrees.</td>
</tr>
<tr>
<td><code>points_per_second</code></td>
<td>int</td>
<td>1500</td>
<td>Points generated by all lasers per second.</td>
</tr>
<tr>
<td><code>range</code></td>
<td>float</td>
<td>100</td>
<td>Maximum distance to measure/raycast in meters.</td>
</tr>
<tr>
<td><code>sensor_tick</code></td>
<td>float</td>
<td>0.0</td>
<td>Simulation seconds between sensor captures (ticks).</td>
</tr>
<tr>
<td><code>vertical_fov</code></td>
<td>float</td>
<td>30.0</td>
<td>Vertical field of view in degrees.</td>
</tr>
</tbody>
</table>
<p><br></p>
<h4 id="output-attributes_7">Output attributes</h4>
<table>
<thead>
<tr>
<th>Sensor data attribute</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>raw_data</code></td>
<td><a href="../python_api#carlaradardetection">carla.RadarDetection</a></td>
<td>The list of points detected.</td>
</tr>
</tbody>
</table>
<p><br></p>
<table>
<thead>
<tr>
<th>RadarDetection attributes</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>altitude</code></td>
<td>float</td>
<td>Altitude angle in radians.</td>
</tr>
<tr>
<td><code>azimuth</code></td>
<td>float</td>
<td>Azimuth angle in radians.</td>
</tr>
<tr>
<td><code>depth</code></td>
<td>float</td>
<td>Distance in meters.</td>
</tr>
<tr>
<td><code>velocity</code></td>
<td>float</td>
<td>Velocity towards the sensor.</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="rgb-camera">RGB camera</h2>
<ul>
<li><strong>Blueprint:</strong> sensor.camera.rgb</li>
<li><strong>Output:</strong> <a href="../python_api/#carla.Image">carla.Image</a> per step (unless <code>sensor_tick</code> says otherwise)..</li>
</ul>
<p>The "RGB" camera acts as a regular camera capturing images from the scene.
<a href="../python_api/#carla.ColorConverter">carla.colorConverter</a></p>
<p>If <code>enable_postprocess_effects</code> is enabled, a set of post-process effects is applied to the image for the sake of realism:</p>
<ul>
<li><strong>Vignette:</strong> darkens the border of the screen.</li>
<li><strong>Grain jitter:</strong> adds some noise to the render.</li>
<li><strong>Bloom:</strong> intense lights burn the area around them.</li>
<li><strong>Auto exposure:</strong> modifies the image gamma to simulate the eye adaptation to darker or brighter areas.</li>
<li><strong>Lens flares:</strong> simulates the reflection of bright objects on the lens.</li>
<li><strong>Depth of field:</strong> blurs objects near or very far away of the camera.</li>
</ul>
<p>The <code>sensor_tick</code> tells how fast we want the sensor to capture the data.
A value of 1.5 means that we want the sensor to capture data each second and a half. By default a value of 0.0 means as fast as possible.</p>
<p><img alt="ImageRGB" src="../img/ref_sensors_rgb.jpg" /></p>
<h4 id="basic-camera-attributes_1">Basic camera attributes</h4>
<p><br></p>
<table>
<thead>
<tr>
<th>Blueprint attribute</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>bloom_intensity</code></td>
<td>float</td>
<td>0.675</td>
<td>Intensity for the bloom post-process effect, <code>0.0</code> for disabling it.</td>
</tr>
<tr>
<td><code>fov</code></td>
<td>float</td>
<td>90.0</td>
<td>Horizontal field of view in degrees.</td>
</tr>
<tr>
<td><code>fstop</code></td>
<td>float</td>
<td>1.4</td>
<td>Opening of the camera lens. Aperture is <code>1/fstop</code> with typical lens going down to f/1.2 (larger opening). Larger numbers will reduce the Depth of Field effect.</td>
</tr>
<tr>
<td><code>image_size_x</code></td>
<td>int</td>
<td>800</td>
<td>Image width in pixels.</td>
</tr>
<tr>
<td><code>image_size_y</code></td>
<td>int</td>
<td>600</td>
<td>Image height in pixels.</td>
</tr>
<tr>
<td><code>iso</code></td>
<td>float</td>
<td>100.0</td>
<td>The camera sensor sensitivity.</td>
</tr>
<tr>
<td><code>gamma</code></td>
<td>float</td>
<td>2.2</td>
<td>Target gamma value of the camera.</td>
</tr>
<tr>
<td><code>lens_flare_intensity</code></td>
<td>float</td>
<td>0.1</td>
<td>Intensity for the lens flare post-process effect, <code>0.0</code> for disabling it.</td>
</tr>
<tr>
<td><code>sensor_tick</code></td>
<td>float</td>
<td>0.0</td>
<td>Simulation seconds between sensor captures (ticks).</td>
</tr>
<tr>
<td><code>shutter_speed</code></td>
<td>float</td>
<td>200.0</td>
<td>The camera shutter speed in seconds (1.0/s).</td>
</tr>
</tbody>
</table>
<h4 id="camera-lens-distortion-attributes_1">Camera lens distortion attributes</h4>
<p><br></p>
<table>
<thead>
<tr>
<th>Blueprint attribute</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>lens_circle_falloff</code></td>
<td>float</td>
<td>5.0</td>
<td>Range: [0.0, 10.0]</td>
</tr>
<tr>
<td><code>lens_circle_multiplier</code></td>
<td>float</td>
<td>0.0</td>
<td>Range: [0.0, 10.0]</td>
</tr>
<tr>
<td><code>lens_k</code></td>
<td>float</td>
<td>-1.0</td>
<td>Range: [-inf, inf]</td>
</tr>
<tr>
<td><code>lens_kcube</code></td>
<td>float</td>
<td>0.0</td>
<td>Range: [-inf, inf]</td>
</tr>
<tr>
<td><code>lens_x_size</code></td>
<td>float</td>
<td>0.08</td>
<td>Range: [0.0, 1.0]</td>
</tr>
<tr>
<td><code>lens_y_size</code></td>
<td>float</td>
<td>0.08</td>
<td>Range: [0.0, 1.0]</td>
</tr>
</tbody>
</table>
<h4 id="advanced-camera-attributes">Advanced camera attributes</h4>
<p>Since these effects are provided by UE, please make sure to check their documentation:</p>
<ul>
<li><a href="https://docs.unrealengine.com/en-US/Engine/Rendering/PostProcessEffects/AutomaticExposure/index.html">Automatic Exposure</a></li>
<li><a href="https://docs.unrealengine.com/en-US/Engine/Rendering/PostProcessEffects/DepthOfField/CinematicDOFMethods/index.html">Cinematic Depth of Field Method</a></li>
<li><a href="https://docs.unrealengine.com/en-US/Engine/Rendering/PostProcessEffects/ColorGrading/index.html">Color Grading and Filmic Tonemapper</a></li>
</ul>
<table>
<thead>
<tr>
<th>Blueprint attribute</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>min_fstop</code></td>
<td>float</td>
<td>1.2</td>
<td>Maximum aperture.</td>
</tr>
<tr>
<td><code>blade_count</code></td>
<td>int</td>
<td>5</td>
<td>Number of blades that make up the diaphragm mechanism.</td>
</tr>
<tr>
<td><code>exposure_mode</code></td>
<td>str</td>
<td><code>histogram</code></td>
<td>Can be <code>manual</code> or <code>histogram</code>. More in <a href="https://docs.unrealengine.com/en-US/Engine/Rendering/PostProcessEffects/AutomaticExposure/index.html">UE4 docs</a>.</td>
</tr>
<tr>
<td><code>exposure_compensation</code></td>
<td>float</td>
<td><strong>Linux:</strong> +0.75<br><strong>Windows:</strong> 0.0</td>
<td>Logarithmic adjustment for the exposure. 0: no adjustment, -1:2x darker, -2:4 darker, 1:2x brighter, 2:4x brighter.</td>
</tr>
<tr>
<td><code>exposure_min_bright</code></td>
<td>float</td>
<td>10.0</td>
<td>In <code>exposure_mode: "histogram"</code>. Minimum brightness for auto exposure. The lowest the eye can adapt within. Must be greater than 0 and less than or equal to <code>exposure_max_bright</code>.</td>
</tr>
<tr>
<td><code>exposure_max_bright</code></td>
<td>float</td>
<td>12.0</td>
<td>In `exposure_mode: "histogram"`. Maximum brightness for auto exposure. The highestthe eye can adapt within. Must be greater than 0 and greater than or equal to `exposure_min_bright`.</td>
</tr>
<tr>
<td><code>exposure_speed_up</code></td>
<td>float</td>
<td>3.0</td>
<td>In <code>exposure_mode: "histogram"</code>. Speed at which the adaptation occurs from dark to bright environment.</td>
</tr>
<tr>
<td><code>exposure_speed_down</code></td>
<td>float</td>
<td>1.0</td>
<td>In <code>exposure_mode: "histogram"</code>. Speed at which the adaptation occurs from bright to dark environment.</td>
</tr>
<tr>
<td><code>calibration_constant</code></td>
<td>float</td>
<td>16.0</td>
<td>Calibration constant for 18% albedo.</td>
</tr>
<tr>
<td><code>focal_distance</code></td>
<td>float</td>
<td>1000.0</td>
<td>Distance at which the depth of field effect should be sharp. Measured in cm (UE units).</td>
</tr>
<tr>
<td><code>blur_amount</code></td>
<td>float</td>
<td>1.0</td>
<td>Strength/intensity of motion blur.</td>
</tr>
<tr>
<td><code>blur_radius</code></td>
<td>float</td>
<td>0.0</td>
<td>Radius in pixels at 1080p resolution to emulate atmospheric scattering according to distance from camera.</td>
</tr>
<tr>
<td><code>motion_blur_intensity</code></td>
<td>float</td>
<td>0.45</td>
<td>Strength of motion blur [0,1].</td>
</tr>
<tr>
<td><code>motion_blur_max_distortion</code></td>
<td>float</td>
<td>0.35</td>
<td>Max distortion caused by motion blur. Percentage of screen width.</td>
</tr>
<tr>
<td><code>motion_blur_min_object_screen_size</code></td>
<td>float</td>
<td>0.1</td>
<td>Percentage of screen width objects must have for motion blur, lower value means less draw calls.</td>
</tr>
<tr>
<td><code>slope</code></td>
<td>float</td>
<td>0.88</td>
<td>Steepness of the S-curve for the tonemapper. Larger values make the slope steeper (darker) [0.0, 1.0].</td>
</tr>
<tr>
<td><code>toe</code></td>
<td>float</td>
<td>0.55</td>
<td>Adjusts dark color in the tonemapper [0.0, 1.0].</td>
</tr>
<tr>
<td><code>shoulder</code></td>
<td>float</td>
<td>0.26</td>
<td>Adjusts bright color in the tonemapper [0.0, 1.0].</td>
</tr>
<tr>
<td><code>black_clip</code></td>
<td>float</td>
<td>0.0</td>
<td>This should NOT be adjusted. Sets where the crossover happens and black tones start to cut off their value [0.0, 1.0].</td>
</tr>
<tr>
<td><code>white_clip</code></td>
<td>float</td>
<td>0.04</td>
<td>Set where the crossover happens and white tones start to cut off their value. Subtle change in most cases [0.0, 1.0].</td>
</tr>
<tr>
<td><code>temp</code></td>
<td>float</td>
<td>6500.0</td>
<td>White balance in relation to the temperature of the light in the scene. <strong>White light:</strong> when this matches light temperature. <strong>Warm light:</strong> When higher than the light in the scene, it is a yellowish color. <strong>Cool light:</strong> When lower than the light. Blueish color.</td>
</tr>
<tr>
<td><code>tint</code></td>
<td>float</td>
<td>0.0</td>
<td>White balance temperature tint. Adjusts cyan and magenta color ranges. This should be used along with the white balance Temp property to get accurate colors. Under some light temperatures, the colors may appear to be more yellow or blue. This can be used to balance the resulting color to look more natural.</td>
</tr>
<tr>
<td><code>chromatic_aberration_intensity</code></td>
<td>float</td>
<td>0.0</td>
<td>Scaling factor to control color shifting, more noticeable on the screen borders.</td>
</tr>
<tr>
<td><code>chromatic_aberration_offset</code></td>
<td>float</td>
<td>0.0</td>
<td>Normalized distance to the center of the image where the effect takes place.</td>
</tr>
<tr>
<td><code>enable_postprocess_effects</code></td>
<td>bool</td>
<td>True</td>
<td>Post-process effects activation.</td>
</tr>
</tbody>
</table>
<p><br></p>
<h4 id="output-attributes_8">Output attributes</h4>
<table>
<thead>
<tr>
<th>Sensor data attribute</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>frame</code></td>
<td>int</td>
<td>Frame number when the measurement took place.</td>
</tr>
<tr>
<td><code>timestamp</code></td>
<td>double</td>
<td>Simulation time of the measurement in seconds since the beginning of the episode.</td>
</tr>
<tr>
<td><code>transform</code></td>
<td><a href="../python_api#carlatransform">carla.Transform</a></td>
<td>Location and rotation in world coordinates of the sensor at the time of the measurement.</td>
</tr>
<tr>
<td><code>width</code></td>
<td>int</td>
<td>Image width in pixels.</td>
</tr>
<tr>
<td><code>height</code></td>
<td>int</td>
<td>Image height in pixels.</td>
</tr>
<tr>
<td><code>fov</code></td>
<td>float</td>
<td>Horizontal field of view in degrees.</td>
</tr>
<tr>
<td><code>raw_data</code></td>
<td>bytes</td>
<td>Array of BGRA 32-bit pixels.</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="rss-sensor">RSS sensor</h2>
<ul>
<li><strong>Blueprint:</strong> sensor.other.rss</li>
<li><strong>Output:</strong> <a href="../python_api/#carla.RssResponse">carla.RssResponse</a> per step (unless <code>sensor_tick</code> says otherwise).</li>
</ul>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>It is highly recommended to read the specific <a href="../adv_rss/">rss documentation</a> before reading this.</p>
</div>
<p>This sensor integrates the <a href="https://github.com/intel/ad-rss-lib">C++ Library for Responsibility Sensitive Safety</a> in CARLA. It is disabled by default in CARLA, and it has to be explicitly built in order to be used.</p>
<p>The RSS sensor calculates the RSS state of a vehicle and retrieves the current RSS Response as sensor data. The <a href="../python_api/#carla.RssRestrictor">carla.RssRestrictor</a> will use this data to adapt a <a href="../python_api/#carla.VehicleControl">carla.VehicleControl</a> before applying it to a vehicle.</p>
<p>These controllers can be generated by an <em>Automated Driving</em> stack or user input. For instance, hereunder there is a fragment of code from <code>PythonAPI/examples/rss/manual_control_rss.py</code>, where the user input is modified using RSS when necessary.</p>
<p><strong>1.</strong> Checks if the <strong>RssSensor</strong> generates a valid response containing restrictions.
<strong>2.</strong> Gathers the current dynamics of the vehicle and the vehicle physics.
<strong>3.</strong> Applies restrictions to the vehicle control using the response from the RssSensor, and the current dynamics and physicis of the vehicle.</p>
<pre><code class="language-py">rss_proper_response = self._world.rss_sensor.proper_response if self._world.rss_sensor and self._world.rss_sensor.response_valid else None
if rss_proper_response:
...
        vehicle_control = self._restrictor.restrict_vehicle_control(
            vehicle_control, rss_proper_response, self._world.rss_sensor.ego_dynamics_on_route, self._vehicle_physics)
</code></pre>
<h4 id="the-carlarsssensor-class">The carla.RssSensor class</h4>
<p>The blueprint for this sensor has no modifiable attributes. However, the <a href="../python_api/#carla.RssSensor">carla.RssSensor</a> object that it instantiates has attributes and methods that are detailed in the Python API reference. Here is a summary of them.</p>
<table>
<thead>
<tr>
<th><a href="../python_api#carlarsssensor">carla.RssSensor variables</a></th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>ego_vehicle_dynamics</code></td>
<td><a href="https://intel.github.io/ad-rss-lib/ad_rss/Appendix-ParameterDiscussion/">ad.rss.world.RssDynamics</a></td>
<td>RSS parameters to be applied for the ego vehicle</td>
</tr>
<tr>
<td><code>other_vehicle_dynamics</code></td>
<td><a href="https://intel.github.io/ad-rss-lib/ad_rss/Appendix-ParameterDiscussion/">ad.rss.world.RssDynamics</a></td>
<td>RSS parameters to be applied for the other vehicles</td>
</tr>
<tr>
<td><code>pedestrian_dynamics</code></td>
<td><a href="https://intel.github.io/ad-rss-lib/ad_rss/Appendix-ParameterDiscussion/">ad.rss.world.RssDynamics</a></td>
<td>RSS parameters to be applied for pedestrians</td>
</tr>
<tr>
<td><code>road_boundaries_mode</code></td>
<td><a href="../python_api#carlarssroadboundariesmode">carla.RssRoadBoundariesMode</a></td>
<td>Enables/Disables the <a href="https://intel.github.io/ad-rss-lib/ad_rss_map_integration/HandleRoadBoundaries">stay on road</a> feature. Default is <strong>Off</strong>.</td>
</tr>
</tbody>
</table>
<p><br></p>
<pre><code class="language-py"># Fragment of rss_sensor.py
# The carla.RssSensor is updated when listening for a new carla.RssResponse
def _on_rss_response(weak_self, response):
...
        self.timestamp = response.timestamp
        self.response_valid = response.response_valid
        self.proper_response = response.proper_response
        self.ego_dynamics_on_route = response.ego_dynamics_on_route
        self.rss_state_snapshot = response.rss_state_snapshot
        self.situation_snapshot = response.situation_snapshot
        self.world_model = response.world_model
</code></pre>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This sensor works fully on the client side. There is no blueprint in the server. Changes on the attributes will have effect <strong>after</strong> the <em>listen()</em> has been called.</p>
</div>
<p>The methods available in this class are related to the routing of the vehicle. RSS calculations are always based on a route of the ego vehicle through the road network.</p>
<p>The sensor allows to control the considered route by providing some key points, which could be the <a href="../python_api/#carla.Transform">carla.Transform</a> in a <a href="../python_api/#carla.Waypoint">carla.Waypoint</a>. These points are best selected after the intersections to force the route to take the desired turn.</p>
<table>
<thead>
<tr>
<th><a href="../python_api#carlarsssensor">carla.RssSensor methods</a></th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>routing_targets</code></td>
<td>Get the current list of routing targets used for route.</td>
</tr>
<tr>
<td><code>append_routing_target</code></td>
<td>Append an additional position to the current routing targets.</td>
</tr>
<tr>
<td><code>reset_routing_targets</code></td>
<td>Deletes the appended routing targets.</td>
</tr>
<tr>
<td><code>drop_route</code></td>
<td>Discards the current route and creates a new one.</td>
</tr>
<tr>
<td><code>register_actor_constellation_callback</code></td>
<td>Register a callback to customize the calculations.</td>
</tr>
<tr>
<td><code>set_log_level</code></td>
<td>Sets the log level.</td>
</tr>
<tr>
<td><code>set_map_log_level</code></td>
<td>Sets the log level used for map related logs.</td>
</tr>
</tbody>
</table>
<p><br></p>
<hr />
<pre><code class="language-py"># Update the current route
self.sensor.reset_routing_targets()
if routing_targets:
    for target in routing_targets:
        self.sensor.append_routing_target(target)
</code></pre>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If no routing targets are defined, a random route is created.</p>
</div>
<h4 id="output-attributes_9">Output attributes</h4>
<table>
<thead>
<tr>
<th><a href="../python_api#carlarssresponse">carla.RssResponse attributes</a></th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>response_valid</code></td>
<td>bool</td>
<td>Validity of the response data.</td>
</tr>
<tr>
<td><code>proper_response</code></td>
<td><a href="https://intel.github.io/ad-rss-lib/doxygen/ad_rss/structad_1_1rss_1_1state_1_1ProperResponse.html">ad.rss.state.ProperResponse</a></td>
<td>Proper response that the RSS calculated for the vehicle including acceleration restrictions.</td>
</tr>
<tr>
<td><code>rss_state_snapshot</code></td>
<td><a href="https://intel.github.io/ad-rss-lib/doxygen/ad_rss/structad_1_1rss_1_1state_1_1RssStateSnapshot.html">ad.rss.state.RssStateSnapshot</a></td>
<td>RSS states at the current point in time. This is the detailed individual output of the RSS calclulations.</td>
</tr>
<tr>
<td><code>situation_snapshot</code></td>
<td><a href="https://intel.github.io/ad-rss-lib/doxygen/ad_rss/structad_1_1rss_1_1situation_1_1SituationSnapshot.html">ad.rss.situation.SituationSnapshot</a></td>
<td>RSS situation at the current point in time. This is the processed input data for the RSS calclulations.</td>
</tr>
<tr>
<td><code>world_model</code></td>
<td><a href="https://intel.github.io/ad-rss-lib/doxygen/ad_rss/structad_1_1rss_1_1world_1_1WorldModel.html">ad.rss.world.WorldModel</a></td>
<td>RSS world model at the current point in time. This is the input data for the RSS calculations.</td>
</tr>
<tr>
<td><code>ego_dynamics_on_route</code></td>
<td><a href="../python_api#carlarssegodynamicsonroute">carla.RssEgoDynamicsOnRoute</a></td>
<td>Current ego vehicle dynamics regarding the route.</td>
</tr>
</tbody>
</table>
<p>In case a actor_constellation_callback is registered, a call is triggered for:</p>
<ol>
<li>default calculation (<code>actor_constellation_data.other_actor=None</code>)</li>
<li>per-actor calculation</li>
</ol>
<pre><code class="language-py"># Fragment of rss_sensor.py
# The function is registered as actor_constellation_callback
def _on_actor_constellation_request(self, actor_constellation_data):
    actor_constellation_result = carla.RssActorConstellationResult()
    actor_constellation_result.rss_calculation_mode = ad.rss.map.RssMode.NotRelevant
    actor_constellation_result.restrict_speed_limit_mode = ad.rss.map.RssSceneCreation.RestrictSpeedLimitMode.IncreasedSpeedLimit10
    actor_constellation_result.ego_vehicle_dynamics = self.current_vehicle_parameters
    actor_constellation_result.actor_object_type = ad.rss.world.ObjectType.Invalid
    actor_constellation_result.actor_dynamics = self.current_vehicle_parameters

    actor_id = -1
    actor_type_id = &quot;none&quot;
    if actor_constellation_data.other_actor != None:
        # customize actor_constellation_result for specific actor
        ...
    else:
        # default
        ...
    return actor_constellation_result
</code></pre>
<hr />
<h2 id="semantic-lidar-sensor">Semantic LIDAR sensor</h2>
<ul>
<li><strong>Blueprint:</strong> sensor.lidar.ray_cast_semantic</li>
<li><strong>Output:</strong> <a href="../python_api/#carla.SemanticLidarMeasurement">carla.SemanticLidarMeasurement</a> per step (unless <code>sensor_tick</code> says otherwise).</li>
</ul>
<p>This sensor simulates a rotating LIDAR implemented using ray-casting that exposes all the information about the raycast hit. Its behaviour is quite similar to the <a href="#lidar-sensor">LIDAR sensor</a>, but there are two main differences between them.</p>
<ul>
<li>The raw data retrieved by the semantic LIDAR includes more data per point.<ul>
<li>Coordinates of the point (as the normal LIDAR does).</li>
<li>The cosine between the angle of incidence and the normal of the surface hit.</li>
<li>Instance and semantic ground-truth. Basically the index of the CARLA object hit, and its semantic tag.</li>
</ul>
</li>
<li>The semantic LIDAR does not include neither intensity, drop-off nor noise model attributes.</li>
</ul>
<p>The points are computed by adding a laser for each channel distributed in the vertical FOV. The rotation is simulated computing the horizontal angle that the LIDAR rotated in a frame. The point cloud is calculated by doing a ray-cast for each laser in every step.</p>
<pre><code class="language-sh">points_per_channel_each_step = points_per_second / (FPS * channels)
</code></pre>
<p>A LIDAR measurement contains a package with all the points generated during a <code>1/FPS</code> interval. During this interval the physics are not updated so all the points in a measurement reflect the same "static picture" of the scene.</p>
<p>This output contains a cloud of lidar semantic detections and therefore, it can be iterated to retrieve a list of their <a href="../python_api/#carla.SemanticLidarDetection"><code>carla.SemanticLidarDetection</code></a>:</p>
<pre><code class="language-py">for detection in semantic_lidar_measurement:
    print(detection)
</code></pre>
<p>The rotation of the LIDAR can be tuned to cover a specific angle on every simulation step (using a <a href="../adv_synchrony_timestep/">fixed time-step</a>). For example, to rotate once per step (full circle output, as in the picture below), the rotation frequency and the simulated FPS should be equal. <br>
<strong>1.</strong> Set the sensor's frequency <code>sensors_bp['lidar'][0].set_attribute('rotation_frequency','10')</code>. <br>
<strong>2.</strong> Run the simulation using <code>python3 config.py --fps=10</code>.</p>
<p><img alt="LidarPointCloud" src="../img/semantic_lidar_point_cloud.jpg" /></p>
<h4 id="semanticlidar-attributes">SemanticLidar attributes</h4>
<p><br></p>
<table>
<thead>
<tr>
<th>Blueprint attribute</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>channels</code></td>
<td>int</td>
<td>32</td>
<td>Number of lasers.</td>
</tr>
<tr>
<td><code>range</code></td>
<td>float</td>
<td>10.0</td>
<td>Maximum distance to measure/raycast in meters (centimeters for CARLA 0.9.6 or previous).</td>
</tr>
<tr>
<td><code>points_per_second</code></td>
<td>int</td>
<td>56000</td>
<td>Points generated by all lasers per second.</td>
</tr>
<tr>
<td><code>rotation_frequency</code></td>
<td>float</td>
<td>10.0</td>
<td>LIDAR rotation frequency.</td>
</tr>
<tr>
<td><code>upper_fov</code></td>
<td>float</td>
<td>10.0</td>
<td>Angle in degrees of the highest laser.</td>
</tr>
<tr>
<td><code>lower_fov</code></td>
<td>float</td>
<td>-30.0</td>
<td>Angle in degrees of the lowest laser.</td>
</tr>
<tr>
<td><code>horizontal_fov</code></td>
<td>float</td>
<td>360.0</td>
<td>Horizontal field of view in degrees, 0 - 360.</td>
</tr>
<tr>
<td><code>sensor_tick</code></td>
<td>float</td>
<td>0.0</td>
<td>Simulation seconds between sensor captures (ticks).</td>
</tr>
</tbody>
</table>
<p><br></p>
<h4 id="output-attributes_10">Output attributes</h4>
<table>
<thead>
<tr>
<th>Sensor data attribute</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>frame</code></td>
<td>int</td>
<td>Frame number when the measurement took place.</td>
</tr>
<tr>
<td><code>timestamp</code></td>
<td>double</td>
<td>Simulation time of the measurement in seconds since the beginning of the episode.</td>
</tr>
<tr>
<td><code>transform</code></td>
<td><a href="../python_api#carlatransform">carla.Transform</a></td>
<td>Location and rotation in world coordinates of the sensor at the time of the measurement.</td>
</tr>
<tr>
<td><code>horizontal_angle</code></td>
<td>float</td>
<td>Angle (radians) in the XY plane of the LIDAR in the current frame.</td>
</tr>
<tr>
<td><code>channels</code></td>
<td>int</td>
<td>Number of channels (lasers) of the LIDAR.</td>
</tr>
<tr>
<td><code>get_point_count(channel)</code></td>
<td>int</td>
<td>Number of points per channel captured in the current frame.</td>
</tr>
<tr>
<td><code>raw_data</code></td>
<td>bytes</td>
<td>Array containing the point cloud with instance and semantic information. For each point, four 32-bits floats are stored. <br>  XYZ coordinates. <br> cosine of the incident angle. <br> Unsigned int containing the index of the object hit. <br>  Unsigned int containing the semantic tag of the object it.</td>
</tr>
</tbody>
</table>
<h2 id="semantic-segmentation-camera">Semantic segmentation camera</h2>
<ul>
<li><strong>Blueprint:</strong> sensor.camera.semantic_segmentation</li>
<li><strong>Output:</strong> <a href="../python_api/#carla.Image">carla.Image</a> per step (unless <code>sensor_tick</code> says otherwise).</li>
</ul>
<p>This camera classifies every object in sight by displaying it in a different color according to its tags (e.g., pedestrians in a different color than vehicles).
When the simulation starts, every element in scene is created with a tag. So it happens when an actor is spawned. The objects are classified by their relative file path in the project. For example, meshes stored in <code>Unreal/CarlaUE4/Content/Static/Pedestrians</code> are tagged as <code>Pedestrian</code>.</p>
<p><img alt="ImageSemanticSegmentation" src="../img/ref_sensors_semantic.jpg" /></p>
<p>The server provides an image with the tag information <strong>encoded in the red channel</strong>: A pixel with a red value of <code>x</code> belongs to an object with tag <code>x</code>.
This raw <a href="../python_api/#carla.Image">carla.Image</a> can be stored and converted it with the help of <strong>CityScapesPalette</strong>  in <a href="../python_api/#carla.ColorConverter">carla.ColorConverter</a> to apply the tags information and show picture with the semantic segmentation.</p>
<pre><code class="language-py">...
raw_image.save_to_disk(&quot;path/to/save/converted/image&quot;,carla.cityScapesPalette)
</code></pre>
<p>The following tags are currently available:</p>
<table>
<thead>
<tr>
<th>Value</th>
<th>Tag</th>
<th>Converted color</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>0</code></td>
<td>Unlabeled</td>
<td><code>(0, 0, 0)</code></td>
<td>Elements that have not been categorized are considered <code>Unlabeled</code>. This category is meant to be empty or at least contain elements with no collisions.</td>
</tr>
<tr>
<td><code>1</code></td>
<td>Building</td>
<td><code>(70, 70, 70)</code></td>
<td>Buildings like houses, skyscrapers,... and the elements attached to them. <br> E.g. air conditioners, scaffolding, awning or ladders and much more.</td>
</tr>
<tr>
<td><code>2</code></td>
<td>Fence</td>
<td><code>(100, 40, 40)</code></td>
<td>Barriers, railing, or other upright structures. Basically wood or wire assemblies that enclose an area of ground.</td>
</tr>
<tr>
<td><code>3</code></td>
<td>Other</td>
<td><code>(55, 90, 80)</code></td>
<td>Everything that does not belong to any other category.</td>
</tr>
<tr>
<td><code>4</code></td>
<td>Pedestrian</td>
<td><code>(220, 20, 60)</code></td>
<td>Humans that walk or ride/drive any kind of vehicle or mobility system. <br> E.g. bicycles or scooters, skateboards, horses, roller-blades, wheel-chairs, etc.</td>
</tr>
<tr>
<td><code>5</code></td>
<td>Pole</td>
<td><code>(153, 153, 153)</code></td>
<td>Small mainly vertically oriented pole. If the pole has a horizontal part (often for traffic light poles) this is also considered pole. <br> E.g. sign pole, traffic light poles.</td>
</tr>
<tr>
<td><code>6</code></td>
<td>RoadLine</td>
<td><code>(157, 234, 50)</code></td>
<td>The markings on the road.</td>
</tr>
<tr>
<td><code>7</code></td>
<td>Road</td>
<td><code>(128, 64, 128)</code></td>
<td>Part of ground on which cars usually drive. <br> E.g. lanes in any directions, and streets.</td>
</tr>
<tr>
<td><code>8</code></td>
<td>SideWalk</td>
<td><code>(244, 35, 232)</code></td>
<td>Part of ground designated for pedestrians or cyclists. Delimited from the road by some obstacle (such as curbs or poles), not only by markings. This label includes a possibly delimiting curb, traffic islands (the walkable part), and pedestrian zones.</td>
</tr>
<tr>
<td><code>9</code></td>
<td>Vegetation</td>
<td><code>(107, 142, 35)</code></td>
<td>Trees, hedges, all kinds of vertical vegetation. Ground-level vegetation is considered <code>Terrain</code>.</td>
</tr>
<tr>
<td><code>10</code></td>
<td>Vehicles</td>
<td><code>(0, 0, 142)</code></td>
<td>Cars, vans, trucks, motorcycles, bikes, buses, trains.</td>
</tr>
<tr>
<td><code>11</code></td>
<td>Wall</td>
<td><code>(102, 102, 156)</code></td>
<td>Individual standing walls. Not part of a building.</td>
</tr>
<tr>
<td><code>12</code></td>
<td>TrafficSign</td>
<td><code>(220, 220, 0)</code></td>
<td>Signs installed by the state/city authority, usually for traffic regulation. This category does not include the poles where signs are attached to. <br> E.g. traffic- signs, parking signs, direction signs...</td>
</tr>
<tr>
<td><code>13</code></td>
<td>Sky</td>
<td><code>(70, 130, 180)</code></td>
<td>Open sky. Includes clouds and the sun.</td>
</tr>
<tr>
<td><code>14</code></td>
<td>Ground</td>
<td><code>(81, 0, 81)</code></td>
<td>Any horizontal ground-level structures that does not match any other category. For example areas shared by vehicles and pedestrians, or flat roundabouts delimited from the road by a curb.</td>
</tr>
<tr>
<td><code>15</code></td>
<td>Bridge</td>
<td><code>(150, 100, 100)</code></td>
<td>Only the structure of the bridge. Fences, people, vehicles, an other elements on top of it are labeled separately.</td>
</tr>
<tr>
<td><code>16</code></td>
<td>RailTrack</td>
<td><code>(230, 150, 140)</code></td>
<td>All kind of rail tracks that are non-drivable by cars. <br> E.g. subway and train rail tracks.</td>
</tr>
<tr>
<td><code>17</code></td>
<td>GuardRail</td>
<td><code>(180, 165, 180)</code></td>
<td>All types of guard rails/crash barriers.</td>
</tr>
<tr>
<td><code>18</code></td>
<td>TrafficLight</td>
<td><code>(250, 170, 30)</code></td>
<td>Traffic light boxes without their poles.</td>
</tr>
<tr>
<td><code>19</code></td>
<td>Static</td>
<td><code>(110, 190, 160)</code></td>
<td>Elements in the scene and props that are immovable. <br> E.g. fire hydrants, fixed benches, fountains, bus stops, etc.</td>
</tr>
<tr>
<td><code>20</code></td>
<td>Dynamic</td>
<td><code>(170, 120, 50)</code></td>
<td>Elements whose position is susceptible to change over time. <br> E.g. Movable trash bins, buggies, bags, wheelchairs, animals, etc.</td>
</tr>
<tr>
<td><code>21</code></td>
<td>Water</td>
<td><code>(45, 60, 150)</code></td>
<td>Horizontal water surfaces. <br> E.g. Lakes, sea, rivers.</td>
</tr>
<tr>
<td><code>22</code></td>
<td>Terrain</td>
<td><code>(145, 170, 100)</code></td>
<td>Grass, ground-level vegetation, soil or sand. These areas are not meant to be driven on. This label includes a possibly delimiting curb.</td>
</tr>
</tbody>
</table>
<p><br></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Read <a href="../tuto_D_create_semantic_tags/">this</a> tutorial to create new semantic tags. </p>
</div>
<h4 id="basic-camera-attributes_2">Basic camera attributes</h4>
<table>
<thead>
<tr>
<th>Blueprint attribute</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>fov</code></td>
<td>float</td>
<td>90.0</td>
<td>Horizontal field of view in degrees.</td>
</tr>
<tr>
<td><code>image_size_x</code></td>
<td>int</td>
<td>800</td>
<td>Image width in pixels.</td>
</tr>
<tr>
<td><code>image_size_y</code></td>
<td>int</td>
<td>600</td>
<td>Image height in pixels.</td>
</tr>
<tr>
<td><code>sensor_tick</code></td>
<td>float</td>
<td>0.0</td>
<td>Simulation seconds between sensor captures (ticks).</td>
</tr>
</tbody>
</table>
<hr />
<h4 id="camera-lens-distortion-attributes_2">Camera lens distortion attributes</h4>
<table>
<thead>
<tr>
<th>Blueprint attribute</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>lens_circle_falloff</code></td>
<td>float</td>
<td>5.0</td>
<td>Range: [0.0, 10.0]</td>
</tr>
<tr>
<td><code>lens_circle_multiplier</code></td>
<td>float</td>
<td>0.0</td>
<td>Range: [0.0, 10.0]</td>
</tr>
<tr>
<td><code>lens_k</code></td>
<td>float</td>
<td>-1.0</td>
<td>Range: [-inf, inf]</td>
</tr>
<tr>
<td><code>lens_kcube</code></td>
<td>float</td>
<td>0.0</td>
<td>Range: [-inf, inf]</td>
</tr>
<tr>
<td><code>lens_x_size</code></td>
<td>float</td>
<td>0.08</td>
<td>Range: [0.0, 1.0]</td>
</tr>
<tr>
<td><code>lens_y_size</code></td>
<td>float</td>
<td>0.08</td>
<td>Range: [0.0, 1.0]</td>
</tr>
</tbody>
</table>
<hr />
<h4 id="output-attributes_11">Output attributes</h4>
<table>
<thead>
<tr>
<th>Sensor data attribute</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>fov</code></td>
<td>float</td>
<td>Horizontal field of view in degrees.</td>
</tr>
<tr>
<td><code>frame</code></td>
<td>int</td>
<td>Frame number when the measurement took place.</td>
</tr>
<tr>
<td><code>height</code></td>
<td>int</td>
<td>Image height in pixels.</td>
</tr>
<tr>
<td><code>raw_data</code></td>
<td>bytes</td>
<td>Array of BGRA 32-bit pixels.</td>
</tr>
<tr>
<td><code>timestamp</code></td>
<td>double</td>
<td>Simulation time of the measurement in seconds since the beginning of the episode.</td>
</tr>
<tr>
<td><code>transform</code></td>
<td><a href="../python_api#carlatransform">carla.Transform</a></td>
<td>Location and rotation in world coordinates of the sensor at the time of the measurement.</td>
</tr>
<tr>
<td><code>width</code></td>
<td>int</td>
<td>Image width in pixels.</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="dvs-camera">DVS camera</h2>
<ul>
<li><strong>Blueprint:</strong> sensor.camera.dvs</li>
<li><strong>Output:</strong> <a href="../python_api/#carla.DVSEventArray">carla.DVSEventArray</a> per step (unless <code>sensor_tick</code> says otherwise).</li>
</ul>
<p>A Dynamic Vision Sensor (DVS) or Event camera is a sensor that works radically differently from a conventional camera. Instead of capturing
intensity images at a fixed rate, event cameras measure changes of intensity asynchronously, in the form of a stream of events, which encode per-pixel
brightness changes. Event cameras possess distinct properties when compared to standard cameras. They have a very high dynamic range (140 dB
versus 60 dB), no motion blur, and high temporal resolution (in the order of microseconds). Event cameras are thus sensors that can provide high-quality
visual information even in challenging high-speed scenarios and high dynamic range environments, enabling new application domains for vision-based
algorithms.</p>
<p>The DVS camera outputs a stream of events. An event <code>e=(x,y,t,pol)</code> is triggered at a pixel <code>x</code>, <code>y</code> at a timestamp <code>t</code> when the change in
logarithmic intensity <code>L</code> reaches a predefined constant threshold <code>C</code> (typically between 15% and 30%).</p>
<p><code>L(x,y,t) - L(x,y,t-\delta t) = pol C</code></p>
<p><code>t-\delta t</code> is the time when the last event at that pixel was triggered and <code>pol</code> is the polarity of the event according to the sign of the
brightness change. The polarity is positive <code>+1</code> when there is increment in brightness and negative <code>-1</code> when a decrement in brightness occurs. The
working principles depicted in the following figure. The standard camera outputs frames at a fixed rate, thus sending redundant information
when no motion is present in the scene. In contrast, event cameras are data-driven sensors that respond to brightness changes with microsecond
latency. At the plot, a positive (resp. negative) event (blue dot, resp. red dot) is generated whenever the (signed) brightness change exceeds the
contrast threshold <code>C</code> for one dimension <code>x</code> over time <code>t</code>. Observe how the event rate grows when the signal changes rapidly.</p>
<p><img alt="DVSCameraWorkingPrinciple" src="../img/sensor_dvs_scheme.jpg" /></p>
<p>The current implementation of the DVS camera works in a uniform sampling manner between two consecutive synchronous frames. Therefore, in order to
emulate the high temporal resolution (order of microseconds) of a real event camera, the sensor requires to execute at a high frequency (much higher
frequency than a conventional camera). Effectively, the number of events increases the faster a CARLA car drives. Therefore, the sensor frequency
should increase accordingly with the dynamics of the scene. The user should find a balance between time accuracy and computational cost.</p>
<p>The provided script <a href="https://github.com/carla-simulator/carla/blob/master/PythonAPI/examples/manual_control.py"><code>manual_control.py</code></a> uses the DVS camera in order to show how to configure the sensor, how to get the stream of events and how to depict such events in an image format, usually called event frame.</p>
<p>Note that due to the sampling method of the DVS camera, if there is no pixel difference between two consecutive synchronous frames the camera will not return an image. This will always occur in the first frame, as there is no previous frame to compare to and also in the event that there has been no movement between frames. </p>
<p><img alt="DVSCameraWorkingPrinciple" src="../img/sensor_dvs.gif" /></p>
<p>DVS is a camera and therefore has all the attributes available in the RGB camera. Nevertheless, there are few attributes exclusive to the working principle of an Event camera.</p>
<h4 id="dvs-camera-attributes">DVS camera attributes</h4>
<table>
<thead>
<tr>
<th>Blueprint attribute</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>positive_threshold</code></td>
<td>float</td>
<td>0.3</td>
<td>Positive threshold C associated to a increment in brightness change (0-1).</td>
</tr>
<tr>
<td><code>negative_threshold</code></td>
<td>float</td>
<td>0.3</td>
<td>Negative threshold C associated to a decrement in brightness change (0-1).</td>
</tr>
<tr>
<td><code>sigma_positive_threshold</code></td>
<td>float</td>
<td>0</td>
<td>White noise standard deviation for positive events (0-1).</td>
</tr>
<tr>
<td><code>sigma_negative_threshold</code></td>
<td>float</td>
<td>0</td>
<td>White noise standard deviation for negative events (0-1).</td>
</tr>
<tr>
<td><code>refractory_period_ns</code></td>
<td>int</td>
<td>0.0</td>
<td>Refractory period (time during which a pixel cannot fire events just after it fired one), in nanoseconds. It limits the highest frequency of triggering events.</td>
</tr>
<tr>
<td><code>use_log</code></td>
<td>bool</td>
<td>true</td>
<td>Whether to work in the logarithmic intensity scale.</td>
</tr>
<tr>
<td><code>log_eps</code></td>
<td>float</td>
<td>0.001</td>
<td>Epsilon value used to convert images to log: <code>L = log(eps + I / 255.0)</code>.<br>  Where <code>I</code> is the grayscale value of the RGB image: <br><code>I = 0.2989*R + 0.5870*G + 0.1140*B</code>.</td>
</tr>
</tbody>
</table>
<p><br></p>
<hr />
<h2 id="optical-flow-camera">Optical Flow Camera</h2>
<p>The Optical Flow camera captures the motion perceived from the point of view of the camera. Every pixel recorded by this sensor encodes the velocity of that point projected to the image plane. The velocity of a pixel is encoded in the range [-2,2]. To obtain the motion in pixel units, this information can be scaled with the image size to [-2 * image_size, 2 * image_size].</p>
<p><img alt="optical_flow" src="../img/optical_flow.png" /></p>
<h4 id="optical-flow-camera-attributes">Optical Flow camera attributes</h4>
<table>
<thead>
<tr>
<th>Blueprint attribute</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>image_size_x</code></td>
<td>int</td>
<td>800</td>
<td>Image width in pixels.</td>
</tr>
<tr>
<td><code>image_size_y</code></td>
<td>int</td>
<td>600</td>
<td>Image height in pixels.</td>
</tr>
<tr>
<td><code>fov</code></td>
<td>float</td>
<td>90.0</td>
<td>Horizontal field of view in degrees.</td>
</tr>
<tr>
<td><code>sensor_tick</code></td>
<td>float</td>
<td>0.0</td>
<td>Simulation seconds between sensor captures (ticks).</td>
</tr>
</tbody>
</table>
<h4 id="optical-flow-camera-lens-distortion-attributes">Optical Flow camera lens distortion attributes</h4>
<table>
<thead>
<tr>
<th>Blueprint attribute</th>
<th>Type</th>
<th>Default</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>lens_circle_falloff</code></td>
<td>float</td>
<td>5.0</td>
<td>Range: [0.0, 10.0]</td>
</tr>
<tr>
<td><code>lens_circle_multiplier</code></td>
<td>float</td>
<td>0.0</td>
<td>Range: [0.0, 10.0]</td>
</tr>
<tr>
<td><code>lens_k</code></td>
<td>float</td>
<td>-1.0</td>
<td>Range: [-inf, inf]</td>
</tr>
<tr>
<td><code>lens_kcube</code></td>
<td>float</td>
<td>0.0</td>
<td>Range: [-inf, inf]</td>
</tr>
<tr>
<td><code>lens_x_size</code></td>
<td>float</td>
<td>0.08</td>
<td>Range: [0.0, 1.0]</td>
</tr>
<tr>
<td><code>lens_y_size</code></td>
<td>float</td>
<td>0.08</td>
<td>Range: [0.0, 1.0]</td>
</tr>
</tbody>
</table>
<h4 id="output-attributes_12">Output attributes</h4>
<table>
<thead>
<tr>
<th>Sensor data attribute</th>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>frame</code></td>
<td>int</td>
<td>Frame number when the measurement took place.</td>
</tr>
<tr>
<td><code>timestamp</code></td>
<td>double</td>
<td>Simulation time of the measurement in seconds since the beginning of the episode.</td>
</tr>
<tr>
<td><code>transform</code></td>
<td><a href="../python_api#carlatransform">carla.Transform</a></td>
<td>Location and rotation in world coordinates of the sensor at the time of the measurement.</td>
</tr>
<tr>
<td><code>width</code></td>
<td>int</td>
<td>Image width in pixels.</td>
</tr>
<tr>
<td><code>height</code></td>
<td>int</td>
<td>Image height in pixels.</td>
</tr>
<tr>
<td><code>fov</code></td>
<td>float</td>
<td>Horizontal field of view in degrees.</td>
</tr>
<tr>
<td><code>raw_data</code></td>
<td>bytes</td>
<td>Array of BGRA 64-bit pixels containing two float values.</td>
</tr>
</tbody>
</table>
<p><br></p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../ref_recorder_binary_file_format/" class="btn btn-neutral float-left" title="记录器二进制文件格式"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../tutorials/" class="btn btn-neutral float-right" title="教程">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../ref_recorder_binary_file_format/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../tutorials/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../extra.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
