<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Ref sensors - 交通仿真文档</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../extra.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Ref sensors";
        var mkdocs_page_input_path = "ref_sensors.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> 交通仿真文档
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">主页</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">交通仿真文档</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Ref sensors</li>
    <li class="wy-breadcrumbs-aside">
          <a href="https://github.com/OpenHUTB/carla_doc/edit/master/docs/ref_sensors.md" class="icon icon-github"> Edit on GitHub</a>
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="_1"><a href="https://carla.readthedocs.io/en/latest/ref_sensors/">传感器参考</a></h1>
<ul>
<li><a href="#collision-detector"><strong>碰撞检测器</strong></a></li>
<li><a href="#depth-camera"><strong>深度相机</strong></a></li>
<li><a href="#gnss-sensor"><strong>全球导航卫星系统传感器</strong></a></li>
<li><a href="#imu-sensor"><strong>惯性测量单元传感器</strong></a></li>
<li><a href="#lane-invasion-detector"><strong>车道入侵检测器</strong></a></li>
<li><a href="#lidar-sensor"><strong>激光雷达传感器</strong></a></li>
<li><a href="#obstacle-detector"><strong>障碍物检测器</strong></a></li>
<li><a href="#radar-sensor"><strong>雷达传感器</strong></a></li>
<li><a href="#rgb-camera"><strong>RGB相机</strong></a></li>
<li><a href="#rss-sensor"><strong>责任敏感安全传感器</strong></a></li>
<li><a href="#semantic-lidar-sensor"><strong>语义激光雷达传感器</strong></a></li>
<li><a href="#semantic-segmentation-camera"><strong>语义分割相机</strong></a></li>
<li><a href="#instance-segmentation-camera"><strong>实例分割相机</strong></a></li>
<li><a href="#dvs-camera"><strong>动态视觉传感器相机</strong></a></li>
<li><a href="#optical-flow-camera"><strong>光流相机</strong></a></li>
</ul>
<div class="admonition 重要">
<p class="admonition-title">重要</p>
<p>所有传感器都使用虚幻引擎坐标系（<strong>x</strong> - <em>向前</em>，<strong>y</strong> - <em>向右</em>，<strong>z</strong> - <em>向上</em>），并返回本地空间中的坐标。使用任何可视化软件时，请注意其坐标系。许多反转 Y 轴，因此直接可视化传感器数据可能会导致镜像输出。 </p>
</div>
<hr />
<h2 id="_2">碰撞检测器</h2>
<ul>
<li><strong>蓝图：</strong> sensor.other.collision</li>
<li><strong>输出：</strong> 每次碰撞的 <a href="../python_api/#carla.CollisionEvent">carla.CollisionEvent</a> 。</li>
</ul>
<p>每当其父参与者与世界上的某些物体发生碰撞时，该传感器都会记录一个事件。每个碰撞传感器每帧每次碰撞都会产生一个碰撞事件。通过与多个其他参与者的碰撞，可以在单个帧中产生多个碰撞事件。为了确保检测到与任何类型的对象的碰撞，服务器为建筑物或灌木丛等元素创建“假”参与者，以便可以检索语义标签来识别它。</p>
<p>碰撞检测器没有任何可配置的属性。</p>
<h4 id="_3">输出属性</h4>
<table>
<thead>
<tr>
<th>传感器数据属性</th>
<th>类型</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>frame</code></td>
<td>int</td>
<td>进行测量时的帧编号。</td>
</tr>
<tr>
<td><code>timestamp</code></td>
<td>double</td>
<td>自回合开始以来测量的仿真时间（以秒为单位）。</td>
</tr>
<tr>
<td><code>transform</code></td>
<td><a href="../python_api#carlatransform">carla.Transform</a></td>
<td>测量时传感器在世界坐标中的位置和旋转。</td>
</tr>
<tr>
<td><code>actor</code></td>
<td><a href="../python_api#carlaactor">carla.Actor</a></td>
<td>测量碰撞的参与者（传感器的父级）。</td>
</tr>
<tr>
<td><code>other_actor</code></td>
<td><a href="../python_api#carlaactor">carla.Actor</a></td>
<td>与父级相撞的参与者。</td>
</tr>
<tr>
<td><code>normal_impulse</code></td>
<td><a href="../python_api#carlavector3d">carla.Vector3D</a></td>
<td>碰撞的正常脉冲结果。</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="_4">深度相机</h2>
<ul>
<li><strong>蓝图：</strong> sensor.camera.depth</li>
<li><strong>输出：</strong> 每步的图像 <a href="../python_api/#carla.Image">carla.Image</a> （除非<code>sensor_tick</code>另有说明）。</li>
</ul>
<p>相机提供场景的原始数据，编码每个像素到相机的距离（也称为<strong>深度缓冲区</strong>或 <strong>z 缓冲区</strong>）以创建元素的深度图。</p>
<p>该图像使用 RGB 颜色空间的 3 个通道（从低字节到高字节）对每个像素的深度值进行编码：<em>R -&gt; G -&gt; B</em>。以米为单位的实际距离可以通过以下方式解码：</p>
<pre><code>normalized = (R + G * 256 + B * 256 * 256) / (256 * 256 * 256 - 1)
in_meters = 1000 * normalized
</code></pre>
<p>然后，应使用<a href="../python_api/#carla.ColorConverter">carla.colorConverter</a>将输出<a href="../python_api/#carla.Image">carla.Image</a>保存到磁盘，该 <a href="../python_api/#carla.ColorConverter">carla.colorConverter</a> 会将存储在 RGB 通道中的距离转换为包含该距离的 <strong>[0,1]</strong> 浮点数，然后将其转换为灰度。<a href="../python_api/#carla.ColorConverter">carla.colorConverter</a> 中有两个选项可获取深度视图：<strong>Depth</strong> 和 <strong>Logaritmic depth</strong>。两者的精度都是毫米级的，但对数方法可以为更近的物体提供更好的结果。</p>
<pre><code class="language-py">...
raw_image.save_to_disk(&quot;path/to/save/converted/image&quot;,carla.Depth)
</code></pre>
<p><img alt="ImageDepth" src="../img/ref_sensors_depth.jpg" /></p>
<h4 id="_5">相机基本属性</h4>
<table>
<thead>
<tr>
<th>蓝图属性</th>
<th>类型</th>
<th>默认</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>image_size_x</code></td>
<td>int</td>
<td>800</td>
<td>图像宽度（以像素为单位）。</td>
</tr>
<tr>
<td><code>image_size_y</code></td>
<td>int</td>
<td>600</td>
<td>图像高度（以像素为单位）。</td>
</tr>
<tr>
<td><code>fov</code></td>
<td>float</td>
<td>90.0</td>
<td>水平视野（以度为单位）。</td>
</tr>
<tr>
<td><code>sensor_tick</code></td>
<td>float</td>
<td>0.0</td>
<td>传感器捕获之间的仿真秒数（滴答信号）。</td>
</tr>
</tbody>
</table>
<h4 id="_6">相机镜头畸变属性</h4>
<table>
<thead>
<tr>
<th>蓝图属性</th>
<th>类型</th>
<th>默认</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>lens_circle_falloff</code></td>
<td>float</td>
<td>5.0</td>
<td>范围： [0.0, 10.0]</td>
</tr>
<tr>
<td><code>lens_circle_multiplier</code></td>
<td>float</td>
<td>0.0</td>
<td>范围： [0.0, 10.0]</td>
</tr>
<tr>
<td><code>lens_k</code></td>
<td>float</td>
<td>-1.0</td>
<td>范围： [-inf, inf]</td>
</tr>
<tr>
<td><code>lens_kcube</code></td>
<td>float</td>
<td>0.0</td>
<td>范围： [-inf, inf]</td>
</tr>
<tr>
<td><code>lens_x_size</code></td>
<td>float</td>
<td>0.08</td>
<td>范围： [0.0, 1.0]</td>
</tr>
<tr>
<td><code>lens_y_size</code></td>
<td>float</td>
<td>0.08</td>
<td>范围： [0.0, 1.0]</td>
</tr>
</tbody>
</table>
<h4 id="_7">输出属性</h4>
<table>
<thead>
<tr>
<th>传感器数据属性</th>
<th>类型</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>frame</code></td>
<td>int</td>
<td>进行测量时的帧编号。</td>
</tr>
<tr>
<td><code>timestamp</code></td>
<td>double</td>
<td>自回合开始以来测量的仿真时间（以秒为单位）。</td>
</tr>
<tr>
<td><code>transform</code></td>
<td><a href="../python_api#carlatransform">carla.Transform</a></td>
<td>测量时传感器在世界坐标中的位置和旋转。</td>
</tr>
<tr>
<td><code>width</code></td>
<td>int</td>
<td>图像宽度（以像素为单位）。</td>
</tr>
<tr>
<td><code>height</code></td>
<td>int</td>
<td>图像高度（以像素为单位）。</td>
</tr>
<tr>
<td><code>fov</code></td>
<td>float</td>
<td>水平视野（以度为单位）。</td>
</tr>
<tr>
<td><code>raw_data</code></td>
<td>bytes</td>
<td>BGRA 32 位像素阵列。</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="_8">全球导航卫星系统传感器</h2>
<ul>
<li><strong>蓝图：</strong> sensor.other.gnss</li>
<li><strong>Output:</strong> 每一步的全球导航卫星系统的测量 <a href="../python_api/#carla.GnssMeasurement">carla.GNSSMeasurement</a> （<code>sensor_tick</code>另有说明）。</li>
</ul>
<p>报告其父对象的当前 <a href="https://www.gsa.europa.eu/european-gnss/what-gnss">gnss 位置</a> 。这是通过将度量位置添加到 OpenDRIVE 地图定义中定义的初始地理参考位置来计算的。</p>
<h4 id="_9">全球导航卫星系统属性</h4>
<table>
<thead>
<tr>
<th>蓝图属性</th>
<th>类型</th>
<th>默认</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>noise_alt_bias</code></td>
<td>float</td>
<td>0.0</td>
<td>海拔高度噪声模型中的平均参数。</td>
</tr>
<tr>
<td><code>noise_alt_stddev</code></td>
<td>float</td>
<td>0.0</td>
<td>海拔高度噪声模型中的标准偏差参数。</td>
</tr>
<tr>
<td><code>noise_lat_bias</code></td>
<td>float</td>
<td>0.0</td>
<td>纬度噪声模型中的平均参数。</td>
</tr>
<tr>
<td><code>noise_lat_stddev</code></td>
<td>float</td>
<td>0.0</td>
<td>纬度噪声模型中的标准偏差参数。</td>
</tr>
<tr>
<td><code>noise_lon_bias</code></td>
<td>float</td>
<td>0.0</td>
<td>经度噪声模型中的平均参数。</td>
</tr>
<tr>
<td><code>noise_lon_stddev</code></td>
<td>float</td>
<td>0.0</td>
<td>经度噪声模型中的标准偏差参数。</td>
</tr>
<tr>
<td><code>noise_seed</code></td>
<td>int</td>
<td>0</td>
<td>伪随机数生成器的初始化程序。</td>
</tr>
<tr>
<td><code>sensor_tick</code></td>
<td>float</td>
<td>0.0</td>
<td>传感器捕获之间的仿真秒数（滴答信号）。</td>
</tr>
</tbody>
</table>
<p><br></p>
<h4 id="_10">输出属性</h4>
<table>
<thead>
<tr>
<th>传感器数据属性</th>
<th>类型</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>frame</code></td>
<td>int</td>
<td>进行测量时的帧编号。</td>
</tr>
<tr>
<td><code>timestamp</code></td>
<td>double</td>
<td>自回合开始以来测量的仿真时间（以秒为单位）。</td>
</tr>
<tr>
<td><code>transform</code></td>
<td><a href="../python_api#carlatransform">carla.Transform</a></td>
<td>测量时传感器在世界坐标中的位置和旋转。</td>
</tr>
<tr>
<td><code>latitude</code></td>
<td>double</td>
<td>参与者的纬度。</td>
</tr>
<tr>
<td><code>longitude</code></td>
<td>double</td>
<td>参与者的经度。</td>
</tr>
<tr>
<td><code>altitude</code></td>
<td>double</td>
<td>参与者的海拔高度。</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="_11"><a href="https://github.com/carla-simulator/carla/blob/dev/Unreal/CarlaUE4/Plugins/Carla/Source/Carla/Sensor/InertialMeasurementUnit.cpp">惯性测量单元传感器</a></h2>
<ul>
<li><strong>蓝图：</strong> sensor.other.imu</li>
<li><strong>输出：</strong> 每一步的惯性测量单元测量值 <a href="../python_api/#carla.IMUMeasurement">carla.IMUMeasurement</a> （除非传感器滴答信号<code>sensor_tick</code>另有说明）。</li>
</ul>
<p>提供加速度计、陀螺仪和指南针将为父对象检索的测量值。数据是从对象的当前状态收集的。</p>
<h4 id="_12">惯性测量单元属性</h4>
<table>
<thead>
<tr>
<th>蓝图属性</th>
<th>类型</th>
<th>默认</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>noise_accel_stddev_x</code></td>
<td>float</td>
<td>0.0</td>
<td>加速度（X 轴）噪声模型中的标准偏差参数。</td>
</tr>
<tr>
<td><code>noise_accel_stddev_y</code></td>
<td>float</td>
<td>0.0</td>
<td>加速度（Y 轴）噪声模型中的标准偏差参数。</td>
</tr>
<tr>
<td><code>noise_accel_stddev_z</code></td>
<td>float</td>
<td>0.0</td>
<td>加速度（Z 轴）噪声模型中的标准偏差参数。</td>
</tr>
<tr>
<td><code>noise_gyro_bias_x</code></td>
<td>float</td>
<td>0.0</td>
<td>陀螺仪噪声模型中的平均参数（X 轴）。</td>
</tr>
<tr>
<td><code>noise_gyro_bias_y</code></td>
<td>float</td>
<td>0.0</td>
<td>陀螺仪噪声模型中的平均参数（Y 轴）。</td>
</tr>
<tr>
<td><code>noise_gyro_bias_z</code></td>
<td>float</td>
<td>0.0</td>
<td>陀螺仪噪声模型中的平均参数（Z 轴）。</td>
</tr>
<tr>
<td><code>noise_gyro_stddev_x</code></td>
<td>float</td>
<td>0.0</td>
<td>陀螺仪噪声模型中的标准偏差参数（X 轴）。</td>
</tr>
<tr>
<td><code>noise_gyro_stddev_y</code></td>
<td>float</td>
<td>0.0</td>
<td>陀螺仪噪声模型中的标准偏差参数（Y 轴）。</td>
</tr>
<tr>
<td><code>noise_gyro_stddev_z</code></td>
<td>float</td>
<td>0.0</td>
<td>陀螺仪噪声模型中的标准偏差参数（Z 轴）。</td>
</tr>
<tr>
<td><code>noise_seed</code></td>
<td>int</td>
<td>0</td>
<td>伪随机数生成器的初始化程序。</td>
</tr>
<tr>
<td><code>sensor_tick</code></td>
<td>float</td>
<td>0.0</td>
<td>传感器捕获之间的仿真秒数（滴答信号）。</td>
</tr>
</tbody>
</table>
<p><br></p>
<h4 id="_13">输出属性</h4>
<table>
<thead>
<tr>
<th>传感器数据属性</th>
<th>类型</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>frame</code></td>
<td>int</td>
<td>进行测量时的帧编号。</td>
</tr>
<tr>
<td><code>timestamp</code></td>
<td>double</td>
<td>自回合开始以来测量的仿真时间（以秒为单位）。</td>
</tr>
<tr>
<td><code>transform</code></td>
<td><a href="../python_api#carlatransform">carla.Transform</a></td>
<td>测量时传感器在世界坐标中的位置和旋转。</td>
</tr>
<tr>
<td><a href="https://github.com/carla-simulator/carla/blob/dev/Unreal/CarlaUE4/Plugins/Carla/Source/Carla/Sensor/InertialMeasurementUnit.cpp#L102"><code>accelerometer</code></a></td>
<td><a href="../python_api#carlavector3d">carla.Vector3D</a></td>
<td>测量线性加速度（以 <code>m/s^2</code> 为单位）。</td>
</tr>
<tr>
<td><a href="https://github.com/carla-simulator/carla/blob/dev/Unreal/CarlaUE4/Plugins/Carla/Source/Carla/Sensor/InertialMeasurementUnit.cpp#L147"><code>gyroscope</code></a></td>
<td><a href="../python_api#carlavector3d">carla.Vector3D</a></td>
<td>测量角速度（以 <code>rad/sec</code> 为单位）。</td>
</tr>
<tr>
<td><a href="https://github.com/carla-simulator/carla/blob/dev/Unreal/CarlaUE4/Plugins/Carla/Source/Carla/Sensor/InertialMeasurementUnit.cpp#L167"><code>compass</code></a></td>
<td>float</td>
<td>以弧度为单位的方向。在虚幻引擎中北是 <code>(0.0, -1.0, 0.0)</code> 。</td>
</tr>
</tbody>
</table>
<ul>
<li>加速度计算方法：
$$ d_2 (i) = -2.0 \times [ { y_1 \over {h_1 \times h_2 } }  -  { y_2 \over { h_2 \times (h_1+h_2) } }  -  {y_0 \over { h_1 \times (h_1 + h_2) }} ] .$$</li>
</ul>
<p>其中，(h_1) 为当前时间增量，(h_2)为前一个时间增量。</p>
<hr />
<h2 id="_14">车道侵入检测器</h2>
<ul>
<li><strong>蓝图：</strong> sensor.other.lane_invasion</li>
<li><strong>输出：</strong> 每次交叉路口的 <a href="../python_api/#carla.LaneInvasionEvent">carla.LaneInvasionEvent</a> 。</li>
</ul>
<p>每次其父级穿过车道标记时都会注册一个事件。传感器使用地图的 OpenDRIVE 描述提供的道路数据，通过考虑车轮之间的空间来确定主车辆是否正在侵入另一车道。然而，有一些事情需要考虑：</p>
<ul>
<li>OpenDRIVE 文件和地图之间的差异将导致不规则现象，例如在地图中不可见的交叉车道。</li>
<li>输出检索交叉车道标记列表：计算在 OpenDRIVE 中完成，并将四个车轮之间的整个空间视为一个整体。因此，可能有不止一条车道同时穿过。</li>
</ul>
<p>该传感器没有任何可配置属性。</p>
<div class="admonition 重要">
<p class="admonition-title">重要</p>
<p>该传感器完全在客户端工作。</p>
</div>
<h4 id="_15">输出属性</h4>
<table>
<thead>
<tr>
<th>传感器数据属性</th>
<th>类型</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>frame</code></td>
<td>int</td>
<td>进行测量时的帧编号。</td>
</tr>
<tr>
<td><code>timestamp</code></td>
<td>double</td>
<td>自回合开始以来测量的仿真时间（以秒为单位）。</td>
</tr>
<tr>
<td><code>transform</code></td>
<td><a href="../python_api#carlatransform">carla.Transform</a></td>
<td>测量时传感器在世界坐标中的位置和旋转。</td>
</tr>
<tr>
<td><code>actor</code></td>
<td><a href="../python_api#carlaactor">carla.Actor</a></td>
<td>侵入另一车道的车辆（父参与者）。</td>
</tr>
<tr>
<td><code>crossed_lane_markings</code></td>
<td>list(<a href="../python_api#carlalanemarking">carla.LaneMarking</a>)</td>
<td>已穿越的车道标记列表。</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="_16">激光雷达传感器</h2>
<ul>
<li><strong>蓝图：</strong> sensor.lidar.ray_cast</li>
<li><strong>输出：</strong> 每一步 <a href="../python_api/#carla.LidarMeasurement">carla.LidarMeasurement</a> （除非<code>sensor_tick</code> 另有说明）。</li>
</ul>
<p>激光雷达测量包含一个包，其中包含在某个时间间隔内生成的所有点1/FPS。在此间隔期间，物理不会更新，因此测量中的所有点都反映场景的相同“静态图片”。
<code>points_per_channel_each_step = points_per_second / (FPS * channels)</code></p>
<p>此输出包含仿真点云，因此可以对其进行迭代以检索它们的列表 <a href="../python_api/#carla.Location"><code>carla.Location</code></a>：</p>
<pre><code class="language-py">for location in lidar_measurement:
    print(location)
</code></pre>
<p>激光雷达测量的信息被编码为 4D 点。前三个是 xyz 坐标中的空间点，最后一个是旅行过程中的强度损失。该强度通过以下公式计算。
<br>
<img alt="LidarIntensityComputation" src="../img/lidar_intensity.jpg" /></p>
<p><code>a</code> — 衰减系数。这可能取决于传感器的波长和大气条件。可以使用激光雷达属性对其进行修改<code>atmosphere_attenuation_rate</code>。 
<code>d</code> — 从击中点到传感器的距离</p>
<p>为了获得更好的真实感，可以删除云中的点。这是仿真外部扰动造成的损失的简单方法。这可以结合两个不同的来完成。</p>
<ul>
<li><strong>General drop-off</strong> — 随机掉落的分数比例。这是在跟踪之前完成的，这意味着不会计算被丢弃的点，从而提高性能。如果是<code>dropoff_general_rate = 0.5</code>，则扣掉一半的分数。</li>
<li><strong>Instensity-based drop-off</strong> — 对于检测到的每个点，根据计算的强度的概率执行额外的下降。该概率由两个参数确定。<code>dropoff_zero_intensity</code>是强度为零的点被丢弃的概率。<code>dropoff_intensity_limit</code>是阈值强度，超过该阈值将不会掉落任何分数。范围内的点被丢弃的概率是基于这两个参数的线性比例。</li>
</ul>
<p>此外，该<code>noise_stddev</code>属性还使噪声模型能够仿真现实传感器中出现的意外偏差。对于正值，每个点都会沿着激光射线的矢量随机扰动。结果是激光雷达传感器具有完美的角度定位，但距离测量存在噪音。</p>
<p>可以调整激光雷达的旋转以覆盖每个仿真步骤的特定角度（使用 <a href="../adv_synchrony_timestep/">固定的时间步长</a> ）。例如，每步旋转一次（整圈输出，如下图），旋转频率和仿真的 FPS 应该相等。 <br> <strong>1.</strong> 设置传感器的频率 <code>sensors_bp['lidar'][0].set_attribute('rotation_frequency','10')</code>. <br> <strong>2.</strong> 使用 <code>python3 config.py --fps=10</code> 运行仿真。</p>
<p><img alt="LidarPointCloud" src="../img/lidar_point_cloud.jpg" /></p>
<h4 id="_17">激光雷达属性</h4>
<table>
<thead>
<tr>
<th>蓝图属性</th>
<th>类型</th>
<th>默认</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>channels</code></td>
<td>int</td>
<td>32</td>
<td>激光器数量。</td>
</tr>
<tr>
<td><code>range</code></td>
<td>float</td>
<td>10.0</td>
<td>测量/光线投射的最大距离以米为单位（Carla 0.9.6 或更低版本为厘米）。</td>
</tr>
<tr>
<td><code>points_per_second</code></td>
<td>int</td>
<td>56000</td>
<td>所有激光器每秒生成的点。</td>
</tr>
<tr>
<td><code>rotation_frequency</code></td>
<td>float</td>
<td>10.0</td>
<td>激光雷达旋转频率。</td>
</tr>
<tr>
<td><code>upper_fov</code></td>
<td>float</td>
<td>10.0</td>
<td>最高激光的角度（以度为单位）。</td>
</tr>
<tr>
<td><code>lower_fov</code></td>
<td>float</td>
<td>-30.0</td>
<td>最低激光的角度（以度为单位）。</td>
</tr>
<tr>
<td><code>horizontal_fov</code></td>
<td>float</td>
<td>360.0</td>
<td>水平视野（以度为单位），0 - 360。</td>
</tr>
<tr>
<td><code>atmosphere_attenuation_rate</code></td>
<td>float</td>
<td>0.004</td>
<td>测量每米激光雷达强度损失的系数。检查上面的强度计算。</td>
</tr>
<tr>
<td><code>dropoff_general_rate</code></td>
<td>float</td>
<td>0.45</td>
<td>随机丢弃的点的一般比例。</td>
</tr>
<tr>
<td><code>dropoff_intensity_limit</code></td>
<td>float</td>
<td>0.8</td>
<td>对于基于强度的下降，强度阈值，高于该值则不会下降任何点。</td>
</tr>
<tr>
<td><code>dropoff_zero_intensity</code></td>
<td>float</td>
<td>0.4</td>
<td>对于基于强度的下降，每个强度为零的点被下降的概率。</td>
</tr>
<tr>
<td><code>sensor_tick</code></td>
<td>float</td>
<td>0.0</td>
<td>传感器捕获之间的仿真秒数（滴答信号）。</td>
</tr>
<tr>
<td><code>noise_stddev</code></td>
<td>float</td>
<td>0.0</td>
<td>噪声模型的标准偏差，用于干扰沿其光线投射矢量的每个点。</td>
</tr>
</tbody>
</table>
<h4 id="_18">输出属性</h4>
<table>
<thead>
<tr>
<th>传感器数据属性</th>
<th>类型</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>frame</code></td>
<td>int</td>
<td>进行测量时的帧编号。</td>
</tr>
<tr>
<td><code>timestamp</code></td>
<td>double</td>
<td>自回合开始以来测量的仿真时间（以秒为单位）。</td>
</tr>
<tr>
<td><code>transform</code></td>
<td><a href="../python_api#carlatransform">carla.Transform</a></td>
<td>测量时传感器在世界坐标中的位置和旋转。</td>
</tr>
<tr>
<td><code>horizontal_angle</code></td>
<td>float</td>
<td>当前帧中激光雷达的 XY 平面中的角度（弧度）。</td>
</tr>
<tr>
<td><code>channels</code></td>
<td>int</td>
<td>激光雷达的通道（激光器）数量。</td>
</tr>
<tr>
<td><code>get_point_count(channel)</code></td>
<td>int</td>
<td>每个通道捕获此帧的点数。</td>
</tr>
<tr>
<td><code>raw_data</code></td>
<td>bytes</td>
<td>32 位浮点数组（每个点的 XYZI）。</td>
</tr>
</tbody>
</table>
<p><br></p>
<h2 id="_19">障碍物检测器</h2>
<ul>
<li><strong>蓝图：</strong> sensor.other.obstacle</li>
<li><strong>输出：</strong> 每个障碍物的 <a href="../python_api/#carla.ObstacleDetectionEvent">carla.ObstacleDetectionEvent</a> （除非<code>sensor_tick</code>另有说明）。</li>
</ul>
<p>每当父级参与者前方有障碍时，都会注册一个事件。为了预测障碍物，传感器在母车前方创建一个胶囊形状，并用它来检查碰撞。为了确保检测到与任何类型的对象的碰撞，服务器为建筑物或灌木丛等元素创建“假”参与者，以便可以检索语义标签来识别它。</p>
<table>
<thead>
<tr>
<th>蓝图属性</th>
<th>类型</th>
<th>默认</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>distance</code></td>
<td>float</td>
<td>5</td>
<td>轨迹距离。</td>
</tr>
<tr>
<td><code>hit_radius</code></td>
<td>float</td>
<td>0.5</td>
<td>轨迹的半径。</td>
</tr>
<tr>
<td><code>only_dynamics</code></td>
<td>bool</td>
<td>False</td>
<td>如果为 true，则轨迹将仅考虑动态对象。</td>
</tr>
<tr>
<td><code>debug_linetrace</code></td>
<td>bool</td>
<td>False</td>
<td>如果为 true，则轨迹将可见。</td>
</tr>
<tr>
<td><code>sensor_tick</code></td>
<td>float</td>
<td>0.0</td>
<td>传感器捕获之间的仿真秒数（滴答信号）。</td>
</tr>
</tbody>
</table>
<p><br></p>
<h4 id="_20">输出属性</h4>
<table>
<thead>
<tr>
<th>传感器数据属性</th>
<th>类型</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>frame</code></td>
<td>int</td>
<td>进行测量时的帧编号。</td>
</tr>
<tr>
<td><code>timestamp</code></td>
<td>double</td>
<td>自回车开始以来测量的仿真时间（以秒为单位）。</td>
</tr>
<tr>
<td><code>transform</code></td>
<td><a href="../python_api#carlatransform">carla.Transform</a></td>
<td>测量时传感器在世界坐标中的位置和旋转。</td>
</tr>
<tr>
<td><code>actor</code></td>
<td><a href="../python_api#carlaactor">carla.Actor</a></td>
<td>检测到障碍物的参与者（父级参与者）。</td>
</tr>
<tr>
<td><code>other_actor</code></td>
<td><a href="../python_api#carlaactor">carla.Actor</a></td>
<td>参与者被检测为障碍物。</td>
</tr>
<tr>
<td><code>distance</code></td>
<td>float</td>
<td>从参与者 <code>actor</code> 到其他参与者 <code>other_actor</code> 的距离。</td>
</tr>
</tbody>
</table>
<p><br></p>
<h2 id="_21">雷达传感器</h2>
<ul>
<li><strong>蓝图：</strong> sensor.other.radar</li>
<li><strong>输出：</strong> 每一步 <a href="../python_api/#carla.RadarMeasurement">carla.RadarMeasurement</a> （除非<code>sensor_tick</code>另有说明）。</li>
</ul>
<p>传感器创建一个圆锥视图，该视图被转换为视野中的元素及其相对于传感器的速度的二维点图。这可用于塑造元素并评估它们的运动和方向。由于使用极坐标，这些点将集中在视图中心周围。</p>
<p>测量的点作为<a href="../python_api/#carla.RadarDetection">carla.RadarDetection</a>数组包含在<a href="../python_api/#carla.RadarMeasurement">carla.RadarMeasurement</a>中，该数组指定它们的极坐标、距离和速度。雷达传感器提供的原始数据可以轻松转换为 <strong>numpy</strong> 可管理的格式：</p>
<pre><code class="language-py"># 为了获得 numpy [[vel, azimuth, altitude, depth],...[,,,]]:
points = np.frombuffer(radar_data.raw_data, dtype=np.dtype('f4'))
points = np.reshape(points, (len(radar_data), 4))
</code></pre>
<p>提供的脚本<code>manual_control.py</code>使用此传感器来显示正在检测的点，并在静态时将其绘制为白色，在向物体移动时将其绘制为红色，在远离物体时将其绘制为蓝色：</p>
<p><img alt="ImageRadar" src="../img/ref_sensors_radar.jpg" /></p>
<table>
<thead>
<tr>
<th>蓝图属性</th>
<th>类型</th>
<th>默认</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>horizontal_fov</code></td>
<td>float</td>
<td>30.0</td>
<td>水平视野（以度为单位）。</td>
</tr>
<tr>
<td><code>points_per_second</code></td>
<td>int</td>
<td>1500</td>
<td>所有激光器每秒生成的点。</td>
</tr>
<tr>
<td><code>range</code></td>
<td>float</td>
<td>100</td>
<td>测量/光线投射的最大距离（以米为单位）。</td>
</tr>
<tr>
<td><code>sensor_tick</code></td>
<td>float</td>
<td>0.0</td>
<td>传感器捕获之间的仿真秒数（滴答信号）。</td>
</tr>
<tr>
<td><code>vertical_fov</code></td>
<td>float</td>
<td>30.0</td>
<td>垂直视野（以度为单位）。</td>
</tr>
</tbody>
</table>
<p><br></p>
<h4 id="_22">输出属性</h4>
<table>
<thead>
<tr>
<th>传感器数据属性</th>
<th>类型</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>raw_data</code></td>
<td><a href="../python_api#carlaradardetection">carla.RadarDetection</a></td>
<td>检测到的点列表。</td>
</tr>
</tbody>
</table>
<p><br></p>
<table>
<thead>
<tr>
<th>RadarDetection 属性</th>
<th>类型</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>altitude</code></td>
<td>float</td>
<td>以弧度表示的高度角。</td>
</tr>
<tr>
<td><code>azimuth</code></td>
<td>float</td>
<td>方位角（以弧度表示）。</td>
</tr>
<tr>
<td><code>depth</code></td>
<td>float</td>
<td>距离以米为单位。</td>
</tr>
<tr>
<td><code>velocity</code></td>
<td>float</td>
<td>朝向传感器的速度。</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="rgb">RGB 相机</h2>
<ul>
<li><strong>蓝图：</strong> sensor.camera.rgb</li>
<li><strong>输出：</strong> 每一步 <a href="../python_api/#carla.Image">carla.Image</a> （除非<code>sensor_tick</code>另有说明）。</li>
</ul>
<p>“RGB”相机充当捕获场景图像的常规相机。
<a href="../python_api/#carla.ColorConverter">carla.colorConverter</a></p>
<p>如果 <code>enable_postprocess_effects</code> 启用，为了真实感，一组后处理效果将应用于图像：</p>
<ul>
<li><strong>Vignette:</strong> 使屏幕边框变暗。</li>
<li><strong>Grain jitter:</strong> 为渲染添加一些噪点。</li>
<li><strong>Bloom:</strong> 强烈的光线会灼烧它们周围的区域。</li>
<li><strong>Auto exposure:</strong> 修改图像伽玛以仿真眼睛对较暗或较亮区域的适应。</li>
<li><strong>Lens flares:</strong> 仿真明亮物体在镜头上的反射。</li>
<li><strong>Depth of field:</strong> 模糊靠近或远离相机的物体。</li>
</ul>
<p><code>sensor_tick</code>告诉我们希望传感器捕获数据的速度有多快。值为 1.5 意味着我们希望传感器每半秒捕获一次数据。默认情况下，值 0.0 表示尽可能快。</p>
<p><img alt="ImageRGB" src="../img/ref_sensors_rgb.jpg" /></p>
<h4 id="_23">相机基本属性</h4>
<p><br></p>
<table>
<thead>
<tr>
<th>蓝图属性</th>
<th>类型</th>
<th>默认</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>bloom_intensity</code></td>
<td>float</td>
<td>0.675</td>
<td>光晕后处理效果的强度，<code>0.0</code>用于禁用它。</td>
</tr>
<tr>
<td><code>fov</code></td>
<td>float</td>
<td>90.0</td>
<td>水平视野（以度为单位）。</td>
</tr>
<tr>
<td><code>fstop</code></td>
<td>float</td>
<td>1.4</td>
<td>相机镜头的打开。典型镜头的光圈<code>1/fstop</code>为 f/1.2（更大的光圈）。较大的数字将减少景深效果。</td>
</tr>
<tr>
<td><code>image_size_x</code></td>
<td>int</td>
<td>800</td>
<td>图像宽度（以像素为单位）。</td>
</tr>
<tr>
<td><code>image_size_y</code></td>
<td>int</td>
<td>600</td>
<td>图像高度（以像素为单位）。</td>
</tr>
<tr>
<td><code>iso</code></td>
<td>float</td>
<td>100.0</td>
<td>相机传感器的灵敏度。</td>
</tr>
<tr>
<td><code>gamma</code></td>
<td>float</td>
<td>2.2</td>
<td>相机的目标伽玛值。</td>
</tr>
<tr>
<td><code>lens_flare_intensity</code></td>
<td>float</td>
<td>0.1</td>
<td>镜头眩光后处理效果的强度，<code>0.0</code>用于禁用它。</td>
</tr>
<tr>
<td><code>sensor_tick</code></td>
<td>float</td>
<td>0.0</td>
<td>传感器捕获之间的仿真秒数（滴答信号）。</td>
</tr>
<tr>
<td><code>shutter_speed</code></td>
<td>float</td>
<td>200.0</td>
<td>相机快门速度，以秒为单位 (1.0/s)。</td>
</tr>
</tbody>
</table>
<h4 id="_24">相机镜头畸变属性</h4>
<p><br></p>
<table>
<thead>
<tr>
<th>蓝图属性</th>
<th>类型</th>
<th>默认</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>lens_circle_falloff</code></td>
<td>float</td>
<td>5.0</td>
<td>范围： [0.0, 10.0]</td>
</tr>
<tr>
<td><code>lens_circle_multiplier</code></td>
<td>float</td>
<td>0.0</td>
<td>范围： [0.0, 10.0]</td>
</tr>
<tr>
<td><code>lens_k</code></td>
<td>float</td>
<td>-1.0</td>
<td>范围： [-inf, inf]</td>
</tr>
<tr>
<td><code>lens_kcube</code></td>
<td>float</td>
<td>0.0</td>
<td>范围： [-inf, inf]</td>
</tr>
<tr>
<td><code>lens_x_size</code></td>
<td>float</td>
<td>0.08</td>
<td>范围： [0.0, 1.0]</td>
</tr>
<tr>
<td><code>lens_y_size</code></td>
<td>float</td>
<td>0.08</td>
<td>范围： [0.0, 1.0]</td>
</tr>
</tbody>
</table>
<h4 id="_25">高级相机属性</h4>
<p>由于这些效果是由虚幻引擎提供的，请务必检查他们的文档：</p>
<ul>
<li><a href="https://docs.unrealengine.com/en-US/Engine/Rendering/PostProcessEffects/AutomaticExposure/index.html">自动曝光</a></li>
<li><a href="https://docs.unrealengine.com/en-US/Engine/Rendering/PostProcessEffects/DepthOfField/CinematicDOFMethods/index.html">影视级景深方案</a></li>
<li><a href="https://docs.unrealengine.com/en-US/Engine/Rendering/PostProcessEffects/ColorGrading/index.html">颜色分级和电影色调映射器</a></li>
</ul>
<table>
<thead>
<tr>
<th>蓝图属性</th>
<th>类型</th>
<th>默认</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>min_fstop</code></td>
<td>float</td>
<td>1.2</td>
<td>最大光圈。</td>
</tr>
<tr>
<td><code>blade_count</code></td>
<td>int</td>
<td>5</td>
<td>构成隔膜机构的叶片数量。</td>
</tr>
<tr>
<td><code>exposure_mode</code></td>
<td>str</td>
<td><code>histogram</code></td>
<td>可以是 <code>manual</code> 或or <code>histogram</code>。更多内容请参见<a href="https://docs.unrealengine.com/en-US/Engine/Rendering/PostProcessEffects/AutomaticExposure/index.html">UE4 文档</a>。</td>
</tr>
<tr>
<td><code>exposure_compensation</code></td>
<td>float</td>
<td><strong>Linux:</strong> +0.75<br><strong>Windows:</strong> 0.0</td>
<td>曝光的对数调整。0：无调整，-1：2 倍更暗，-2：4 更暗，1：2 倍更亮，2：4 倍更亮。</td>
</tr>
<tr>
<td><code>exposure_min_bright</code></td>
<td>float</td>
<td>10.0</td>
<td>在<code>exposure_mode: "histogram"</code>。自动曝光的最低亮度。眼睛能适应的最低限度。必须大于 0 且小于或等于<code>exposure_max_bright</code>。</td>
</tr>
<tr>
<td><code>exposure_max_bright</code></td>
<td>float</td>
<td>12.0</td>
<td>在`exposure_mode: "histogram"`中。自动曝光的最大亮度。眼睛能适应的最高限度。必须大于 0 且大于或等于 `exposure_min_bright`。</td>
</tr>
<tr>
<td><code>exposure_speed_up</code></td>
<td>float</td>
<td>3.0</td>
<td>在 <code>exposure_mode: "histogram"</code>。。从黑暗环境到明亮环境的适应速度。</td>
</tr>
<tr>
<td><code>exposure_speed_down</code></td>
<td>float</td>
<td>1.0</td>
<td>在 <code>exposure_mode: "histogram"</code>。从明亮环境到黑暗环境的适应速度。</td>
</tr>
<tr>
<td><code>calibration_constant</code></td>
<td>float</td>
<td>16.0</td>
<td>18% 反照率的校准常数。</td>
</tr>
<tr>
<td><code>focal_distance</code></td>
<td>float</td>
<td>1000.0</td>
<td>景深效果应清晰的距离。以厘米（虚幻引擎单位）为单位测量。</td>
</tr>
<tr>
<td><code>blur_amount</code></td>
<td>float</td>
<td>1.0</td>
<td>运动模糊的强度/强度。</td>
</tr>
<tr>
<td><code>blur_radius</code></td>
<td>float</td>
<td>0.0</td>
<td>1080p 分辨率下的半径（以像素为单位），根据距相机的距离仿真大气散射。</td>
</tr>
<tr>
<td><code>motion_blur_intensity</code></td>
<td>float</td>
<td>0.45</td>
<td>运动模糊的强度 [0,1]。</td>
</tr>
<tr>
<td><code>motion_blur_max_distortion</code></td>
<td>float</td>
<td>0.35</td>
<td>运动模糊引起的最大失真。屏幕宽度的百分比。</td>
</tr>
<tr>
<td><code>motion_blur_min_object_screen_size</code></td>
<td>float</td>
<td>0.1</td>
<td>对于运动模糊，对象必须具有屏幕宽度的百分比，较低的值意味着较少的绘制调用。</td>
</tr>
<tr>
<td><code>slope</code></td>
<td>float</td>
<td>0.88</td>
<td>色调映射器 S 曲线的陡度。值越大，斜率越陡（越暗）[0.0, 1.0]。</td>
</tr>
<tr>
<td><code>toe</code></td>
<td>float</td>
<td>0.55</td>
<td>调整色调映射器中的深色 [0.0, 1.0]。</td>
</tr>
<tr>
<td><code>shoulder</code></td>
<td>float</td>
<td>0.26</td>
<td>调整色调映射器中的明亮颜色 [0.0, 1.0]。</td>
</tr>
<tr>
<td><code>black_clip</code></td>
<td>float</td>
<td>0.0</td>
<td>不应调整此值。设置交叉发生和黑色色调开始切断其值的位置 [0.0, 1.0]。</td>
</tr>
<tr>
<td><code>white_clip</code></td>
<td>float</td>
<td>0.04</td>
<td>设置交叉发生的位置，并且白色色调开始切断其值。大多数情况下会有细微的变化 [0.0, 1.0]。</td>
</tr>
<tr>
<td><code>temp</code></td>
<td>float</td>
<td>6500.0</td>
<td>白平衡与场景中光线的温度有关。<strong>白光</strong>：当与光温匹配时。<strong>暖光</strong>：当高于场景中的光线时，呈淡黄色。<strong>冷光</strong>：低于光线时。蓝色。</td>
</tr>
<tr>
<td><code>tint</code></td>
<td>float</td>
<td>0.0</td>
<td>白平衡温度色调。调整青色和洋红色的颜色范围。这应该与白平衡温度属性一起使用以获得准确的颜色。在某些光温下，颜色可能看起来更黄或更蓝。这可用于平衡最终的颜色，使其看起来更自然。</td>
</tr>
<tr>
<td><code>chromatic_aberration_intensity</code></td>
<td>float</td>
<td>0.0</td>
<td>用于控制色彩偏移的缩放因子，在屏幕边框上更明显。</td>
</tr>
<tr>
<td><code>chromatic_aberration_offset</code></td>
<td>float</td>
<td>0.0</td>
<td>到发生效果的图像中心的正则化距离。</td>
</tr>
<tr>
<td><code>enable_postprocess_effects</code></td>
<td>bool</td>
<td>True</td>
<td>后处理效果激活。</td>
</tr>
</tbody>
</table>
<p><br></p>
<h4 id="_26">输出属性</h4>
<table>
<thead>
<tr>
<th>传感器数据属性</th>
<th>类型</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>frame</code></td>
<td>int</td>
<td>进行测量时的帧编号。</td>
</tr>
<tr>
<td><code>timestamp</code></td>
<td>double</td>
<td>自回合开始以来测量的仿真时间（以秒为单位）。</td>
</tr>
<tr>
<td><code>transform</code></td>
<td><a href="../python_api#carlatransform">carla.Transform</a></td>
<td>测量时传感器在世界坐标中的位置和旋转。</td>
</tr>
<tr>
<td><code>width</code></td>
<td>int</td>
<td>图像宽度（以像素为单位）。</td>
</tr>
<tr>
<td><code>height</code></td>
<td>int</td>
<td>图像高度（以像素为单位）。</td>
</tr>
<tr>
<td><code>fov</code></td>
<td>float</td>
<td>水平视野（以度为单位）。</td>
</tr>
<tr>
<td><code>raw_data</code></td>
<td>bytes</td>
<td>BGRA 32 位像素阵列。</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="_27">责任敏感安全传感器</h2>
<ul>
<li><strong>蓝图：</strong> sensor.other.rss</li>
<li><strong>输出：</strong> 每一步 <a href="../python_api/#carla.RssResponse">carla.RssResponse</a> （除非<code>sensor_tick</code>另有说明）。</li>
</ul>
<div class="admonition 重要">
<p class="admonition-title">重要</p>
<p>强烈建议在阅读本文之前先阅读具体的 <a href="../adv_rss/">任敏感安全文档</a>。</p>
</div>
<p>该传感器集成了 Carla 中的 <a href="https://github.com/intel/ad-rss-lib">责任敏感安全 C++ 库</a> 。它在 Carla 中默认被禁用，并且必须显式构建才能使用。</p>
<p>责任敏感安全传感器计算车辆的责任敏感安全状态并检索当前的责任敏感安全响应作为传感器数据。<a href="../python_api/#carla.RssRestrictor">carla.RssRestrictor</a>将使用此数据来调整<a href="../python_api/#carla.VehicleControl">carla.VehicleControl</a> ，然后再将其应用于车辆。</p>
<p>这些控制器可以通过<em>自动驾驶</em>堆栈或用户输入生成。例如，下面有一段来自 的代码片段<code>PythonAPI/examples/rss/manual_control_rss.py</code>，其中在必要时使用责任敏感安全修改用户输入。</p>
<p><strong>1.</strong> 检查 <strong>RssSensor</strong> 是否生成包含限制的有效响应。
<strong>2.</strong> 收集车辆的当前动态和车辆物理特性。
<strong>3.</strong> 使用 RssSensor 的响应以及车辆当前的动态和物理特性对车辆控制施加限制。</p>
<pre><code class="language-py">rss_proper_response = self._world.rss_sensor.proper_response if self._world.rss_sensor and self._world.rss_sensor.response_valid else None
if rss_proper_response:
...
        vehicle_control = self._restrictor.restrict_vehicle_control(
            vehicle_control, rss_proper_response, self._world.rss_sensor.ego_dynamics_on_route, self._vehicle_physics)
</code></pre>
<h4 id="carlarsssensor">carla.RssSensor 类</h4>
<p>该传感器的蓝图没有可修改的属性。但是，它实例化的 <a href="../python_api/#carla.RssSensor">carla.RssSensor</a> 对象具有 Python API 参考中详细介绍的属性和方法。以下是它们的摘要。</p>
<table>
<thead>
<tr>
<th><a href="../python_api#carlarsssensor">carla.RssSensor 变量</a></th>
<th>类型</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>ego_vehicle_dynamics</code></td>
<td><a href="https://intel.github.io/ad-rss-lib/ad_rss/Appendix-ParameterDiscussion/">ad.rss.world.RssDynamics</a></td>
<td>应用于自我车辆的责任敏感安全参数</td>
</tr>
<tr>
<td><code>other_vehicle_dynamics</code></td>
<td><a href="https://intel.github.io/ad-rss-lib/ad_rss/Appendix-ParameterDiscussion/">ad.rss.world.RssDynamics</a></td>
<td>适用于其他车辆的责任敏感安全参数</td>
</tr>
<tr>
<td><code>pedestrian_dynamics</code></td>
<td><a href="https://intel.github.io/ad-rss-lib/ad_rss/Appendix-ParameterDiscussion/">ad.rss.world.RssDynamics</a></td>
<td>适用于行人的责任敏感安全参数</td>
</tr>
<tr>
<td><code>road_boundaries_mode</code></td>
<td><a href="../python_api#carlarssroadboundariesmode">carla.RssRoadBoundariesMode</a></td>
<td>启用/禁用 <a href="https://intel.github.io/ad-rss-lib/ad_rss_map_integration/HandleRoadBoundaries">留在道路上</a> 功能。默认为<strong>关闭</strong>。</td>
</tr>
</tbody>
</table>
<p><br></p>
<pre><code class="language-py"># rss_sensor.py 代码片段
# The carla.RssSensor is updated when listening for a new carla.RssResponse
def _on_rss_response(weak_self, response):
...
        self.timestamp = response.timestamp
        self.response_valid = response.response_valid
        self.proper_response = response.proper_response
        self.ego_dynamics_on_route = response.ego_dynamics_on_route
        self.rss_state_snapshot = response.rss_state_snapshot
        self.situation_snapshot = response.situation_snapshot
        self.world_model = response.world_model
</code></pre>
<div class="admonition 警告">
<p class="admonition-title">警告</p>
<p>该传感器在客户端完全工作。服务器中没有蓝图。对属性的更改将在调用<em>listen()</em> <strong>后</strong> 生效。</p>
</div>
<p>此类中可用的方法与车辆的路线有关。责任敏感安全计算始终基于本车通过道路网络的路线。</p>
<p>传感器允许通过提供一些关键点来控制所考虑的路线，这些关键点可能是<a href="../python_api/#carla.Transform">carla.Transform</a>中的 <a href="../python_api/#carla.Waypoint">carla.Waypoint</a>。最好在交叉点之后选择这些点，以强制路线采取所需的转弯。</p>
<table>
<thead>
<tr>
<th><a href="../python_api#carlarsssensor">carla.RssSensor 方法</a></th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>routing_targets</code></td>
<td>获取用于路由的当前路由目标列表。</td>
</tr>
<tr>
<td><code>append_routing_target</code></td>
<td>将附加位置附加到当前路由目标。</td>
</tr>
<tr>
<td><code>reset_routing_targets</code></td>
<td>删除附加的路由目标。</td>
</tr>
<tr>
<td><code>drop_route</code></td>
<td>放弃当前路由并创建一条新路由。</td>
</tr>
<tr>
<td><code>register_actor_constellation_callback</code></td>
<td>注册回调来自定义计算。</td>
</tr>
<tr>
<td><code>set_log_level</code></td>
<td>设置日志级别。</td>
</tr>
<tr>
<td><code>set_map_log_level</code></td>
<td>设置用于地图相关日志的日志级别。</td>
</tr>
</tbody>
</table>
<p><br></p>
<hr />
<pre><code class="language-py"># 更新当前路线
self.sensor.reset_routing_targets()
if routing_targets:
    for target in routing_targets:
        self.sensor.append_routing_target(target)
</code></pre>
<div class="admonition 笔记">
<p class="admonition-title">笔记</p>
<p>如果未定义路由目标，则会创建随机路由。</p>
</div>
<h4 id="_28">输出属性</h4>
<table>
<thead>
<tr>
<th><a href="../python_api#carlarssresponse">carla.RssResponse 属性</a></th>
<th>类型</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>response_valid</code></td>
<td>bool</td>
<td>响应数据的有效性。</td>
</tr>
<tr>
<td><code>proper_response</code></td>
<td><a href="https://intel.github.io/ad-rss-lib/doxygen/ad_rss/structad_1_1rss_1_1state_1_1ProperResponse.html">ad.rss.state.ProperResponse</a></td>
<td>RSS 为车辆计算的正确响应，包括加速限制。</td>
</tr>
<tr>
<td><code>rss_state_snapshot</code></td>
<td><a href="https://intel.github.io/ad-rss-lib/doxygen/ad_rss/structad_1_1rss_1_1state_1_1RssStateSnapshot.html">ad.rss.state.RssStateSnapshot</a></td>
<td>RSS 状态为当前时间点。这是 RSS 计算的详细的单独输出。</td>
</tr>
<tr>
<td><code>situation_snapshot</code></td>
<td><a href="https://intel.github.io/ad-rss-lib/doxygen/ad_rss/structad_1_1rss_1_1situation_1_1SituationSnapshot.html">ad.rss.situation.SituationSnapshot</a></td>
<td>当前时间点的 RSS 情况。这是用于 RSS 计算的经过处理的输入数据。</td>
</tr>
<tr>
<td><code>world_model</code></td>
<td><a href="https://intel.github.io/ad-rss-lib/doxygen/ad_rss/structad_1_1rss_1_1world_1_1WorldModel.html">ad.rss.world.WorldModel</a></td>
<td>当前时间点的 RSS 世界模型。这是 RSS 计算的输入数据。</td>
</tr>
<tr>
<td><code>ego_dynamics_on_route</code></td>
<td><a href="../python_api#carlarssegodynamicsonroute">carla.RssEgoDynamicsOnRoute</a></td>
<td>关于路线的当前自我车辆动态。</td>
</tr>
</tbody>
</table>
<p>如果注册了 actor_constellation_callback，则会触发以下调用：</p>
<ol>
<li>默认计算 (<code>actor_constellation_data.other_actor=None</code>)</li>
<li>每个参与者的计算</li>
</ol>
<pre><code class="language-py"># rss_sensor.py 代码片段
# 注册该函数为 actor_constellation_callback
def _on_actor_constellation_request(self, actor_constellation_data):
    actor_constellation_result = carla.RssActorConstellationResult()
    actor_constellation_result.rss_calculation_mode = ad.rss.map.RssMode.NotRelevant
    actor_constellation_result.restrict_speed_limit_mode = ad.rss.map.RssSceneCreation.RestrictSpeedLimitMode.IncreasedSpeedLimit10
    actor_constellation_result.ego_vehicle_dynamics = self.current_vehicle_parameters
    actor_constellation_result.actor_object_type = ad.rss.world.ObjectType.Invalid
    actor_constellation_result.actor_dynamics = self.current_vehicle_parameters

    actor_id = -1
    actor_type_id = &quot;none&quot;
    if actor_constellation_data.other_actor != None:
        # 为特定的参与者定制 actor_constellation_result
        ...
    else:
        # 默认
        ...
    return actor_constellation_result
</code></pre>
<hr />
<h2 id="_29">语义激光雷达传感器</h2>
<ul>
<li><strong>蓝图：</strong> sensor.lidar.ray_cast_semantic</li>
<li><strong>输出：</strong> 每步 <a href="../python_api/#carla.SemanticLidarMeasurement">carla.SemanticLidarMeasurement</a> （除非<code>sensor_tick</code>另有说明）。</li>
</ul>
<p>该传感器仿真使用射线投射实现的旋转激光雷达，公开有关射线投射命中的所有信息。它的行为与 <a href="#lidar-sensor">激光雷达传感器</a> 非常相似，但它们之间有两个主要区别。</p>
<ul>
<li>语义激光雷达检索到的原始数据每个点包含更多数据。<ul>
<li>该点的坐标（与普通激光雷达一样）。</li>
<li>入射角与表面法线之间的余弦值。</li>
<li>实例和语义基础事实。基本上是 Carla 对象命中的索引及其语义标签。</li>
</ul>
</li>
<li>语义激光雷达既不包含强度、衰减也不包含噪声模型属性。</li>
</ul>
<p>这些点是通过为垂直 FOV 中分布的每个通道添加激光来计算的。旋转是通过计算激光雷达在一帧中旋转的水平角度来仿真的。点云是通过在每个步骤中对每个激光进行光线投射来计算的。</p>
<pre><code class="language-sh">points_per_channel_each_step = points_per_second / (FPS * channels)
</code></pre>
<p>激光雷达测量包含一个包，其中包含在某个时间 <code>1/FPS</code> 间隔内生成的所有点。在此间隔期间，物理不会更新，因此测量中的所有点都反映场景的相同“静态图片”。</p>
<p>此输出包含激光雷达语义检测云，因此，可以对其进行迭代以检索其列表 <a href="../python_api/#carla.SemanticLidarDetection"><code>carla.SemanticLidarDetection</code></a>：</p>
<pre><code class="language-py">for detection in semantic_lidar_measurement:
    print(detection)
</code></pre>
<p>可以调整激光雷达的旋转以覆盖每个仿真步骤的特定角度（使用 <a href="../adv_synchrony_timestep/">固定的时间步长</a> ）。例如，每步旋转一次（整圈输出，如下图），旋转频率和仿真的FPS应该相等。 <br>
<strong>1.</strong> 设置传感器的频率 <code>sensors_bp['lidar'][0].set_attribute('rotation_frequency','10')</code>。 <br>
<strong>2.</strong> 使用 <code>python3 config.py --fps=10</code> 运行仿真。</p>
<p><img alt="LidarPointCloud" src="../img/semantic_lidar_point_cloud.jpg" /></p>
<h4 id="_30">语义激光雷达属性</h4>
<p><br></p>
<table>
<thead>
<tr>
<th>蓝图属性</th>
<th>类型</th>
<th>默认</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>channels</code></td>
<td>int</td>
<td>32</td>
<td>激光器数量。</td>
</tr>
<tr>
<td><code>range</code></td>
<td>float</td>
<td>10.0</td>
<td>测量/光线投射的最大距离以米为单位（Carla 0.9.6 或更低版本为厘米）。</td>
</tr>
<tr>
<td><code>points_per_second</code></td>
<td>int</td>
<td>56000</td>
<td>所有激光器每秒生成的点。</td>
</tr>
<tr>
<td><code>rotation_frequency</code></td>
<td>float</td>
<td>10.0</td>
<td>激光雷达旋转频率。</td>
</tr>
<tr>
<td><code>upper_fov</code></td>
<td>float</td>
<td>10.0</td>
<td>最高激光的角度（以度为单位）。</td>
</tr>
<tr>
<td><code>lower_fov</code></td>
<td>float</td>
<td>-30.0</td>
<td>最低激光的角度（以度为单位）。</td>
</tr>
<tr>
<td><code>horizontal_fov</code></td>
<td>float</td>
<td>360.0</td>
<td>水平视野（以度为单位），0 - 360。</td>
</tr>
<tr>
<td><code>sensor_tick</code></td>
<td>float</td>
<td>0.0</td>
<td>传感器捕获之间的仿真秒数（滴答信号）。</td>
</tr>
</tbody>
</table>
<p><br></p>
<h4 id="_31">输出属性</h4>
<table>
<thead>
<tr>
<th>传感器数据属性</th>
<th>类型</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>frame</code></td>
<td>int</td>
<td>进行测量时的帧编号。</td>
</tr>
<tr>
<td><code>timestamp</code></td>
<td>double</td>
<td>自回合开始以来测量的仿真时间（以秒为单位）。</td>
</tr>
<tr>
<td><code>transform</code></td>
<td><a href="../python_api#carlatransform">carla.Transform</a></td>
<td>测量时传感器在世界坐标中的位置和旋转。</td>
</tr>
<tr>
<td><code>horizontal_angle</code></td>
<td>float</td>
<td>当前帧中 LIDAR 的 XY 平面中的角度（弧度）。</td>
</tr>
<tr>
<td><code>channels</code></td>
<td>int</td>
<td>LIDAR 的通道（激光器）数量。</td>
</tr>
<tr>
<td><code>get_point_count(channel)</code></td>
<td>int</td>
<td>当前帧中捕获的每个通道的点数。</td>
</tr>
<tr>
<td><code>raw_data</code></td>
<td>bytes</td>
<td>包含具有实例和语义信息的点云的数组。对于每个点，存储四个 32 位浮点数。 <br>  XYZ 坐标。 <br> 入射角的余弦。 <br> Unsigned int 包含命中对象的索引。 <br>  Unsigned int 包含对象 it 的语义标签。</td>
</tr>
</tbody>
</table>
<h2 id="_32">语义分割相机</h2>
<ul>
<li><strong>蓝图：</strong> sensor.camera.semantic_segmentation</li>
<li><strong>输出：</strong> 每步 <a href="../python_api/#carla.Image">carla.Image</a> （除非 <code>sensor_tick</code> 另有说明）。</li>
</ul>
<p>该摄像机根据其标签以不同的颜色显示它，从而对可见的每个物体进行分类（例如，行人与车辆的颜色不同）。当仿真开始时，场景中的每个元素都会使用标签创建。所以当参与者产生时就会发生这种情况。对象按其在项目中的相对文件路径进行分类。例如，存储在中的网格<code>Unreal/CarlaUE4/Content/Static/Pedestrians</code>被标记为<code>Pedestrian</code>。</p>
<p><img alt="ImageSemanticSegmentation" src="../img/ref_sensors_semantic.jpg" /></p>
<p>服务器提供的图像的标签信息 <strong>编码在红色通道中</strong>： 红色值为 的像素<code>x</code>属于带有标签的对象<code>x</code>。这个原始的<a href="../python_api/#carla.Image">carla.Image</a>可以在<a href="../python_api/#carla.ColorConverter">carla.ColorConverter</a> 中的 <strong>CityScapesPalette</strong> 的帮助下存储和转换，以应用标签信息并通过语义分割显示图片。</p>
<pre><code class="language-py">...
raw_image.save_to_disk(&quot;path/to/save/converted/image&quot;,carla.cityScapesPalette)
</code></pre>
<p>目前可以使用以下标签：</p>
<table>
<thead>
<tr>
<th>值</th>
<th>标签</th>
<th>转换后的颜色</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>0</code></td>
<td>Unlabeled</td>
<td><code>(0, 0, 0)</code></td>
<td>考虑尚未分类的元素<code>Unlabeled</code>。该类别应该是空的或至少包含没有冲突的元素。</td>
</tr>
<tr>
<td><code>1</code></td>
<td>Building</td>
<td><code>(70, 70, 70)</code></td>
<td>房屋、摩天大楼等建筑物以及附着在其上的元素。 <br> 例如空调、脚手架、遮阳篷或梯子等等。</td>
</tr>
<tr>
<td><code>2</code></td>
<td>Fence</td>
<td><code>(100, 40, 40)</code></td>
<td>障碍物、栏杆或其他直立结构。基本上是包围地面区域的木材或电线组件。</td>
</tr>
<tr>
<td><code>3</code></td>
<td>Other</td>
<td><code>(55, 90, 80)</code></td>
<td>一切不属于任何其他类别的东西。</td>
</tr>
<tr>
<td><code>4</code></td>
<td>Pedestrian</td>
<td><code>(220, 20, 60)</code></td>
<td>步行或乘坐/驾驶任何类型的车辆或移动系统的人。 <br> 例如自行车或踏板车、滑板、马、旱冰鞋、轮椅等。</td>
</tr>
<tr>
<td><code>5</code></td>
<td>Pole</td>
<td><code>(153, 153, 153)</code></td>
<td>主要为垂直方向的小型杆。如果杆有水平部分（通常用于交通灯杆），则也被视为杆。 <br> 例如标志杆、交通灯杆。</td>
</tr>
<tr>
<td><code>6</code></td>
<td>RoadLine</td>
<td><code>(157, 234, 50)</code></td>
<td>道路上的标记。</td>
</tr>
<tr>
<td><code>7</code></td>
<td>Road</td>
<td><code>(128, 64, 128)</code></td>
<td>汽车通常行驶的部分地面。 <br> 例如任何方向的车道和街道。</td>
</tr>
<tr>
<td><code>8</code></td>
<td>SideWalk</td>
<td><code>(244, 35, 232)</code></td>
<td>指定供行人或骑自行车者使用的地面的一部分。不仅通过标记，还通过一些障碍物（例如路缘石或杆子）将道路与道路分隔开。该标签包括可能划定的路边、交通岛（步行部分）和步行区。</td>
</tr>
<tr>
<td><code>9</code></td>
<td>Vegetation</td>
<td><code>(107, 142, 35)</code></td>
<td>树木、树篱、各种垂直植被。考虑地面植被<code>Terrain</code>。</td>
</tr>
<tr>
<td><code>10</code></td>
<td>Vehicles</td>
<td><code>(0, 0, 142)</code></td>
<td>汽车、货车、卡车、摩托车、自行车、公共汽车、火车。</td>
</tr>
<tr>
<td><code>11</code></td>
<td>Wall</td>
<td><code>(102, 102, 156)</code></td>
<td>独立的立墙。不是建筑物的一部分。</td>
</tr>
<tr>
<td><code>12</code></td>
<td>TrafficSign</td>
<td><code>(220, 220, 0)</code></td>
<td>由州/市当局安装的标志，通常用于交通管制。此类别不包括附有标志的杆。 <br> 例如交通标志、停车标志、方向标志...</td>
</tr>
<tr>
<td><code>13</code></td>
<td>Sky</td>
<td><code>(70, 130, 180)</code></td>
<td>开阔的天空。包括云和太阳。</td>
</tr>
<tr>
<td><code>14</code></td>
<td>Ground</td>
<td><code>(81, 0, 81)</code></td>
<td>与任何其他类别不匹配的任何水平地面结构。例如，车辆和行人共享的区域，或通过路缘与道路分隔的平坦环岛。</td>
</tr>
<tr>
<td><code>15</code></td>
<td>Bridge</td>
<td><code>(150, 100, 100)</code></td>
<td>只有桥的结构。栅栏、人、车辆以及其上的其他元素都被单独标记。</td>
</tr>
<tr>
<td><code>16</code></td>
<td>RailTrack</td>
<td><code>(230, 150, 140)</code></td>
<td>各种非汽车行驶的铁轨。 <br> 例如地铁和火车铁轨。</td>
</tr>
<tr>
<td><code>17</code></td>
<td>GuardRail</td>
<td><code>(180, 165, 180)</code></td>
<td>所有类型的护栏/防撞栏。</td>
</tr>
<tr>
<td><code>18</code></td>
<td>TrafficLight</td>
<td><code>(250, 170, 30)</code></td>
<td>没有灯杆的交通灯箱。</td>
</tr>
<tr>
<td><code>19</code></td>
<td>Static</td>
<td><code>(110, 190, 160)</code></td>
<td>场景中的元素和道具是不可移动的。 <br> 例如消防栓、固定长凳、喷泉、公交车站等。</td>
</tr>
<tr>
<td><code>20</code></td>
<td>Dynamic</td>
<td><code>(170, 120, 50)</code></td>
<td>位置容易随时间变化的元素。 <br> 例如可移动垃圾桶、手推车、袋子、轮椅、动物等。</td>
</tr>
<tr>
<td><code>21</code></td>
<td>Water</td>
<td><code>(45, 60, 150)</code></td>
<td>水平水面。 <br> 例如湖泊、海洋、河流。</td>
</tr>
<tr>
<td><code>22</code></td>
<td>Terrain</td>
<td><code>(145, 170, 100)</code></td>
<td>草、地面植被、土壤或沙子。这些区域不适合行驶。该标签包括可能的限制性路缘石。</td>
</tr>
</tbody>
</table>
<p><br></p>
<div class="admonition 笔记">
<p class="admonition-title">笔记</p>
<p>阅读 <a href="../tuto_D_create_semantic_tags/">本</a> 教程以创建新的语义标签。</p>
</div>
<h4 id="_33">相机基本属性</h4>
<table>
<thead>
<tr>
<th>蓝图属性</th>
<th>类型</th>
<th>默认</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>fov</code></td>
<td>float</td>
<td>90.0</td>
<td>水平视野（以度为单位）。</td>
</tr>
<tr>
<td><code>image_size_x</code></td>
<td>int</td>
<td>800</td>
<td>图像宽度（以像素为单位）。</td>
</tr>
<tr>
<td><code>image_size_y</code></td>
<td>int</td>
<td>600</td>
<td>图像高度（以像素为单位）。</td>
</tr>
<tr>
<td><code>sensor_tick</code></td>
<td>float</td>
<td>0.0</td>
<td>传感器捕获之间的仿真秒数（滴答声）。</td>
</tr>
</tbody>
</table>
<hr />
<h4 id="_34">相机镜头畸变属性</h4>
<table>
<thead>
<tr>
<th>蓝图属性</th>
<th>类型</th>
<th>默认</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>lens_circle_falloff</code></td>
<td>float</td>
<td>5.0</td>
<td>范围： [0.0, 10.0]</td>
</tr>
<tr>
<td><code>lens_circle_multiplier</code></td>
<td>float</td>
<td>0.0</td>
<td>范围： [0.0, 10.0]</td>
</tr>
<tr>
<td><code>lens_k</code></td>
<td>float</td>
<td>-1.0</td>
<td>范围： [-inf, inf]</td>
</tr>
<tr>
<td><code>lens_kcube</code></td>
<td>float</td>
<td>0.0</td>
<td>范围： [-inf, inf]</td>
</tr>
<tr>
<td><code>lens_x_size</code></td>
<td>float</td>
<td>0.08</td>
<td>范围： [0.0, 1.0]</td>
</tr>
<tr>
<td><code>lens_y_size</code></td>
<td>float</td>
<td>0.08</td>
<td>范围： [0.0, 1.0]</td>
</tr>
</tbody>
</table>
<hr />
<h4 id="_35">输出属性</h4>
<table>
<thead>
<tr>
<th>传感器数据属性</th>
<th>类型</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>fov</code></td>
<td>float</td>
<td>水平视野（以度为单位）。</td>
</tr>
<tr>
<td><code>frame</code></td>
<td>int</td>
<td>进行测量时的帧编号。</td>
</tr>
<tr>
<td><code>height</code></td>
<td>int</td>
<td>图像高度（以像素为单位）。</td>
</tr>
<tr>
<td><code>raw_data</code></td>
<td>bytes</td>
<td>BGRA 32 位像素阵列。</td>
</tr>
<tr>
<td><code>timestamp</code></td>
<td>double</td>
<td>自回合开始以来测量的仿真时间（以秒为单位）。</td>
</tr>
<tr>
<td><code>transform</code></td>
<td><a href="../python_api#carlatransform">carla.Transform</a></td>
<td>测量时传感器在世界坐标中的位置和旋转。</td>
</tr>
<tr>
<td><code>width</code></td>
<td>int</td>
<td>图像宽度（以像素为单位）。</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="_36">动态视觉传感器相机</h2>
<ul>
<li><strong>蓝图：</strong> sensor.camera.dvs</li>
<li><strong>输出：</strong> 每步 <a href="../python_api/#carla.DVSEventArray">carla.DVSEventArray</a> （除非<code>sensor_tick</code>另有说明）。</li>
</ul>
<p><a href="https://baike.baidu.com/item/%E5%8A%A8%E6%80%81%E8%A7%86%E8%A7%89%E4%BC%A0%E6%84%9F%E5%99%A8/23490201">动态视觉传感器</a> （Dynamic Vision Sensor, DVS）或事件相机是一种工作方式与传统相机完全不同的传感器。事件相机不是以固定速率捕获强度图像，而是以事件流的形式异步测量强度变化，对每个像素的亮度变化进行编码。与标准摄像机相比，事件摄像机具有独特的属性。它们具有非常高的动态范围（140 dB 与 60 dB）、无运动模糊和高时间分辨率（微秒级）。因此，事件相机是即使在具有挑战性的高速场景和高动态范围环境下也能提供高质量视觉信息的传感器，为基于视觉的算法提供了新的应用领域。</p>
<p>动态视觉传感器摄像机输出事件流。当对数强度的变化达到预定义的恒定阈值（通常在 15% 到 30% 之间）时，在时间戳处的像素处<code>e=(x,y,t,pol)</code>触发事件。</p>
<p><code>L(x,y,t) - L(x,y,t-\delta t) = pol C</code></p>
<p><code>t-\delta t</code> 是该像素上最后一个事件被触发的时间，并且<code>pol</code>是根据亮度变化的符号的事件的极性。<code>+1</code>当亮度增加时极性为正，<code>-1</code>当亮度减少时极性为负。工作原理如下图所示。标准相机以固定速率输出帧，从而在场景中不存在运动时发送冗余信息。相比之下，事件摄像机是数据驱动的传感器，能够以微秒延迟响应亮度变化。在绘图中，只要（带符号的）亮度变化随时间超过<code>C</code>一维的对比度阈值，就会生成正（或负）事件（蓝点、红点） 。观察信号快速变化时事件率如何增长。</p>
<p><img alt="DVSCameraWorkingPrinciple" src="../img/sensor_dvs_scheme.jpg" /></p>
<p>动态视觉传感器摄像机的当前实现在两个连续同步帧之间以统一采样方式工作。因此，为了仿真真实事件相机的高时间分辨率（微秒级），传感器需要以高频率执行（比传统相机的频率高得多）。实际上，Carla 汽车行驶速度越快，事件数量就会增加。因此，传感器频率应随着场景的动态而相应增加。用户应该在时间精度和计算成本之间找到平衡。</p>
<p>提供的脚本 <a href="https://github.com/OpenHUTB/carla_doc/blob/master/src/examples/manual_control.py"><code>manual_control.py</code></a> 使用动态视觉传感器摄像头来展示如何配置传感器、如何获取事件流以及如何以图像格式（通常称为事件框架）描述此类事件。</p>
<p>请注意，由于动态视觉传感器摄像机的采样方法，如果两个连续同步帧之间没有像素差异，摄像机将不会返回图像。这总是发生在第一帧中，因为没有前一帧可供比较，并且在帧之间没有移动的情况下也是如此。</p>
<p><img alt="DVSCameraWorkingPrinciple" src="../img/sensor_dvs.gif" /></p>
<p>动态视觉传感器是一个相机，因此具有 RGB 相机中可用的所有属性。然而，事件摄像机的工作原理几乎没有什么独有的属性。</p>
<h4 id="_37">动态视觉传感器相机属性</h4>
<table>
<thead>
<tr>
<th>蓝图属性</th>
<th>类型</th>
<th>默认</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>positive_threshold</code></td>
<td>float</td>
<td>0.3</td>
<td>与亮度变化增量相关的正阈值 C (0-1)。</td>
</tr>
<tr>
<td><code>negative_threshold</code></td>
<td>float</td>
<td>0.3</td>
<td>与亮度变化减少相关的负阈值 C (0-1)。</td>
</tr>
<tr>
<td><code>sigma_positive_threshold</code></td>
<td>float</td>
<td>0</td>
<td>积极事件的白噪声标准差 (0-1)。</td>
</tr>
<tr>
<td><code>sigma_negative_threshold</code></td>
<td>float</td>
<td>0</td>
<td>负面事件的白噪声标准差 (0-1)。</td>
</tr>
<tr>
<td><code>refractory_period_ns</code></td>
<td>int</td>
<td>0.0</td>
<td>不应期（像素在触发事件后无法触发事件的时间），以纳秒为单位。它限制了触发事件的最高频率。</td>
</tr>
<tr>
<td><code>use_log</code></td>
<td>bool</td>
<td>true</td>
<td>是否以对数强度标度工作。</td>
</tr>
<tr>
<td><code>log_eps</code></td>
<td>float</td>
<td>0.001</td>
<td>用于将图像转换为日志的 Epsilon 值： <code>L = log(eps + I / 255.0)</code>.<br>  其中 <code>I</code> 是 RGB 图像的灰度值： <br><code>I = 0.2989*R + 0.5870*G + 0.1140*B</code>.</td>
</tr>
</tbody>
</table>
<p><br></p>
<hr />
<h2 id="_38">光流相机</h2>
<p>光流相机捕捉从相机的角度感知的运动。该传感器记录的每个像素都对投影到图像平面的该点的速度进行编码。像素的速度在 [-2,2] 范围内编码。为了获得以像素为单位的运动，可以将该信息与图像大小一起缩放至[-2 * image_size, 2 * image_size]。</p>
<p><img alt="optical_flow" src="../img/optical_flow.png" /></p>
<h4 id="_39">光流相机属性</h4>
<table>
<thead>
<tr>
<th>蓝图属性</th>
<th>类型</th>
<th>默认</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>image_size_x</code></td>
<td>int</td>
<td>800</td>
<td>图像宽度（以像素为单位）。</td>
</tr>
<tr>
<td><code>image_size_y</code></td>
<td>int</td>
<td>600</td>
<td>图像高度（以像素为单位）。</td>
</tr>
<tr>
<td><code>fov</code></td>
<td>float</td>
<td>90.0</td>
<td>水平视野（以度为单位）。</td>
</tr>
<tr>
<td><code>sensor_tick</code></td>
<td>float</td>
<td>0.0</td>
<td>传感器捕获之间的仿真秒数（滴答信号）。</td>
</tr>
</tbody>
</table>
<h4 id="_40">光流相机镜头畸变属性</h4>
<table>
<thead>
<tr>
<th>蓝图属性</th>
<th>类型</th>
<th>默认</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>lens_circle_falloff</code></td>
<td>float</td>
<td>5.0</td>
<td>范围： [0.0, 10.0]</td>
</tr>
<tr>
<td><code>lens_circle_multiplier</code></td>
<td>float</td>
<td>0.0</td>
<td>范围： [0.0, 10.0]</td>
</tr>
<tr>
<td><code>lens_k</code></td>
<td>float</td>
<td>-1.0</td>
<td>范围： [-inf, inf]</td>
</tr>
<tr>
<td><code>lens_kcube</code></td>
<td>float</td>
<td>0.0</td>
<td>范围： [-inf, inf]</td>
</tr>
<tr>
<td><code>lens_x_size</code></td>
<td>float</td>
<td>0.08</td>
<td>范围： [0.0, 1.0]</td>
</tr>
<tr>
<td><code>lens_y_size</code></td>
<td>float</td>
<td>0.08</td>
<td>范围： [0.0, 1.0]</td>
</tr>
</tbody>
</table>
<h4 id="_41">输出属性</h4>
<table>
<thead>
<tr>
<th>传感器数据属性</th>
<th>类型</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>frame</code></td>
<td>int</td>
<td>进行测量时的帧编号。</td>
</tr>
<tr>
<td><code>timestamp</code></td>
<td>double</td>
<td>自回合开始以来测量的仿真时间（以秒为单位）。</td>
</tr>
<tr>
<td><code>transform</code></td>
<td><a href="../python_api#carlatransform">carla.Transform</a></td>
<td>测量时传感器在世界坐标中的位置和旋转。</td>
</tr>
<tr>
<td><code>width</code></td>
<td>int</td>
<td>图像宽度（以像素为单位）。</td>
</tr>
<tr>
<td><code>height</code></td>
<td>int</td>
<td>图像高度（以像素为单位）。</td>
</tr>
<tr>
<td><code>fov</code></td>
<td>float</td>
<td>水平视野（以度为单位）。</td>
</tr>
<tr>
<td><code>raw_data</code></td>
<td>bytes</td>
<td>包含两个浮点值的 BGRA 64 位像素数组。</td>
</tr>
</tbody>
</table>
<p><br></p>
              
            </div>
          </div><footer>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="https://github.com/OpenHUTB/carla_doc" class="fa fa-github" style="color: #fcfcfc"> GitHub</a>
        </span>
    
    
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../extra.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
